{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad326bee-88d1-4c8a-b521-c31f2244dbfb",
   "metadata": {},
   "source": [
    "# Proofread in eCREST\n",
    "\n",
    "The files generated by this script will also be able to be opened in CREST original (though some information may be lost if using original CREST.py or .exe)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4b4d1a-b07e-40b3-a362-e3a7dcc1d64c",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Do the following two setup steps regardless of how you will be using this script. \n",
    "\n",
    "### 1. Imports\n",
    "\n",
    "Run the following code cell to import the necessary packages and modules. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73173909-7e4c-4e66-9216-ec31ae11d47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################################################ \n",
    "# Get the latest CREST files for each ID within the target folder (dirname)\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "from sqlite3 import connect as sqlite3_connect\n",
    "from sqlite3 import DatabaseError\n",
    "from igraph import Graph as ig_Graph\n",
    "from igraph import plot as ig_plot\n",
    "from scipy.spatial.distance import cdist\n",
    "from random import choice as random_choice\n",
    "from itertools import combinations\n",
    "from numpy import array, unravel_index, argmin, mean\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "import neuroglancer\n",
    "from webbrowser import open as wb_open\n",
    "from webbrowser import open_new as wb_open_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774c555f-bd8a-4922-85e1-50aaf8ec727a",
   "metadata": {},
   "source": [
    "### 2. Define the 'ecrest' class using functions from CREST.py\n",
    "\n",
    "An instance of this object will be able to:\n",
    "- open an neuroglancer viewer for proofrieading (see \"Proofread using CREST\")\n",
    "    - add-remove segments (using graph feature for efficiency)\n",
    "    - format itself and save itself as a CREST-style .json\n",
    "- convert from neuroglancer json (see \"Convert From Neuroglancer to eCREST\")\n",
    "    - format itself and save itself as a CREST-style .json\n",
    "    \n",
    "Run the following code cell to define the crest_json class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c77ccc8-da3d-4f40-bee1-8c5d1dcfbd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ecrest:\n",
    "    \n",
    "    def __init__(self,settings_dict, segment_id = None, segment_list = None, filepath = None, launch_viewer=False):\n",
    "        \n",
    "        '''\n",
    "        At some point, store these initialization values (ie addresses, keys lists) as a 'params' file that can be provided to the init function instead of hard-coding\n",
    "        \n",
    "        main_base_id : base segment ID from neuroglancer list\n",
    "        \n",
    "        db_path : filepath to agglomeration database sql file locally on computer\n",
    "        \n",
    "        '''\n",
    "        #####\n",
    "        \n",
    "        '''\n",
    "        set up local stuff\n",
    "        '''\n",
    "        \n",
    "        self.import_from_settings_dict(settings_dict)\n",
    "        self.launch_viewer = launch_viewer\n",
    "        \n",
    "        # Create the connection to the database (right now just for 'Cell Reconstruction')\n",
    "        self.connect_db(self.db_paths)\n",
    "        \n",
    "        # Set up addresses\n",
    "        # addresses are stored in the agglomo SQL file        \n",
    "        required_addresses = ['agglo_address', 'base_address', 'em_address', 'cloud_storage_address']\n",
    "        [self.agglo_seg, self.base_seg, self.em, self.cloud_storage_address]  = self.get_addresses(required_addresses)\n",
    "       \n",
    "        self.cell_pos = 0\n",
    "        self.start_time = time()\n",
    "        self.link_opened = False\n",
    "        \n",
    "        self.get_vx_sizes()\n",
    "        self.added_keybindings = set()\n",
    "        \n",
    "        if segment_id!=None: \n",
    "            # initialize from segment ID\n",
    "            self.load_from_segment_id(segment_id,segment_list)\n",
    "            \n",
    "        if filepath!=None:\n",
    "            # initialize from file (crest .json state)\n",
    "            self.load_from_file(filepath)\n",
    "        #####\n",
    "        \n",
    "        '''\n",
    "        Set up neuroglancer viewer\n",
    "        '''\n",
    "        \n",
    "        if launch_viewer==True:\n",
    "            self.viewer = neuroglancer.Viewer()\n",
    "            self.viewer.set_state({})\n",
    "\n",
    "            # Add keybindings:\n",
    "            pr_keybindings = {\n",
    "                'change-structure': lambda s: self.change_cell_structure(),\n",
    "                'change-anchor-seg': self.change_anchor_seg,\n",
    "                'add-or-remove-seg': self.add_or_remove_seg,\n",
    "                'mark-branch-in-colour': self.mark_branch_in_colour,\n",
    "                'change-point' : lambda s: self.change_point()\n",
    "                # 'grow-graph': lambda s: self.grow_graph(), ### WHY IS GROW GRAPH DISABLED? SEEMS USEFUL FOR LABELING CELL STRUCTURES AFTER ADDING SEGMENTS?\n",
    "                # 'increase-threshold': lambda s: self.increase_threshold(),\n",
    "                # 'decrease-threshold': lambda s: self.decrease_threshold(),\n",
    "                # 'start-branch-focus': self.branch_focus,\n",
    "                # 'accept-new-segs': lambda s: self.accept_new_segs(),        \n",
    "            }\n",
    "\n",
    "            self.add_keybindings_no_duplicates(pr_keybindings)            \n",
    "\n",
    "            with self.viewer.config_state.txn() as s:\n",
    "                s.input_event_bindings.viewer['keyc'] = 'change-structure'\n",
    "                s.input_event_bindings.data_view['dblclick0'] = 'add-or-remove-seg'\n",
    "                s.input_event_bindings.data_view['alt+mousedown0'] = 'mark-branch-in-colour'\n",
    "                s.input_event_bindings.data_view['shift+mousedown2'] = 'change-anchor-seg'\n",
    "                s.input_event_bindings.data_view['keyp'] = 'change-point'\n",
    "                # s.input_event_bindings.viewer['keyg'] = 'grow-graph'\n",
    "                # s.input_event_bindings.viewer['keyk'] = 'increase-threshold'\n",
    "                # s.input_event_bindings.viewer['keyj'] = 'decrease-threshold'\n",
    "                # s.input_event_bindings.viewer['keya'] = 'accept-new-segs'\n",
    "                # s.input_event_bindings.data_view['shift+mousedown0'] = 'start-branch-focus'\n",
    "\n",
    "            with self.viewer.config_state.txn() as s:\n",
    "                s.show_layer_panel = True ###\n",
    "                \n",
    "            # setup point annoations\n",
    "            self.set_endpoint_annotation_layers()\n",
    "            self.set_base_seg_merger_layer()\n",
    "            self.point_pos = -1\n",
    "            self.change_point()\n",
    "\n",
    "            self.set_seg_colours()\n",
    "            self.cell_structure_pos = -1\n",
    "            self.change_cell_structure()\n",
    "\n",
    "            loc = self.get_locations_from_base_segs([self.cell_data['metadata']['main_seg']['base']])[self.cell_data['metadata']['main_seg']['base']]\n",
    "            self.change_view(loc, css=0.22398, ps=389.338)\n",
    "            self.reset_seg_pr_layers()\n",
    "\n",
    "            b = self.cell_data['base_segments']\n",
    "            second_part = ', '.join([f'{x}: {len(b[x])}' for x in b.keys()])\n",
    "            print(f'updating viewer status message: Current Base Segment Counts: {second_part}')\n",
    "            with self.viewer.config_state.txn() as s:\n",
    "                s.status_messages['current_seg_count'] = f'Current Base Segment Counts: {second_part}'\n",
    "\n",
    "            self.open_ng_link()\n",
    "            self.assert_segs_in_sync()\n",
    "\n",
    "    def open_ng_link(self):\n",
    "\n",
    "        if not self.link_opened:\n",
    "            wb_open(str(self.viewer))\n",
    "            self.link_opened = True\n",
    "\n",
    "    def import_from_settings_dict(self,settings_dict):\n",
    "        # self.settings_dict = settings_dict\n",
    "        self.db_paths = Path(settings_dict['db_path'])\n",
    "        self.point_types = settings_dict['annotation_points']\n",
    "        self.cell_structures = settings_dict['cell_structures']\n",
    "        self.max_num_base_added = settings_dict['max_num_base_added']\n",
    "        self.save_dir = Path(settings_dict['save_dir'])\n",
    "\n",
    "    def add_keybindings_no_duplicates(self, dict):\n",
    "\n",
    "        for k in dict:\n",
    "\n",
    "            if k not in self.added_keybindings:\n",
    "\n",
    "                self.viewer.actions.add(k, dict[k])\n",
    "                self.added_keybindings.add(k)           \n",
    "                \n",
    "    def load_from_segment_id(self,main_base_id,segment_list = None):\n",
    "        '''\n",
    "        initializes the graph after loading all the base segments in the agglomeration segment with the main_base_id\n",
    "        '''\n",
    "        agglo_seg_id = self.get_agglo_seg_of_base_seg(str(main_base_id))\n",
    "        \n",
    "        self.cell_data = {\n",
    "            'graph_edges': [],\n",
    "            'graph_nodes': [],\n",
    "            'base_locations': {},\n",
    "            'added_graph_edges': [], \n",
    "            'added_graph_edges_pre_proofreading': [],\n",
    "            'end_points': {key: [] for key in self.point_types},\n",
    "            'base_seg_merge_points': [],\n",
    "            'removed_base_segs': set(),\n",
    "            'anchor_seg' : str(main_base_id),\n",
    "            'metadata': {   \n",
    "                'main_seg' : {'agglo' : {self.agglo_seg : agglo_seg_id}, 'base' : str(main_base_id)},\n",
    "                'data_sources': {\n",
    "                    'em' : self.em, \n",
    "                    'base': self.base_seg, \n",
    "                    'agglo': self.agglo_seg,\n",
    "                    },\n",
    "                'cell-type' : {'manual': [], 'auto': []}\n",
    "                },\n",
    "            'base_segments' : {dtype: set() for dtype in self.cell_structures}\n",
    "        }\n",
    "        \n",
    "        if segment_list!=None: # Then a list of base_segments has been provided and should override getting all base segments from agglomo\n",
    "                                # For example, this would be the case if converting from a neuroglancer-direct json state reconstruction\n",
    "            self.cell_data['base_segments']['unknown']=set(segment_list)\n",
    "\n",
    "            '''\n",
    "            TODO\n",
    "\n",
    "            CHANGE SEGMENT LIST TO BE A DICTIONARY OF {CELL STRUCTURE : LIST} \n",
    "            so can import segments for specific parts of cells\n",
    "\n",
    "            '''\n",
    "\n",
    "        if segment_list==None: # Then a list of base segments was not provided, and need to get all base segments associated with main_base_seg in its agglo segment\n",
    "            segment_list = self.get_base_segs_of_agglo_seg(agglo_seg_id)\n",
    "            self.cell_data['base_segments']['unknown']=set(segment_list)\n",
    "        \n",
    "        # Initialize graph of base_segments (can be across different cell structures)\n",
    "        self.create_pr_graph()\n",
    "        \n",
    "        # Initialize the CREST json file\n",
    "        # self.save_cell_graph()\n",
    "        \n",
    "        print(f'Created a CREST instance for NEW Reconstruction of {main_base_id}. No file saved yet -- save manually.')                                            \n",
    "        \n",
    "    def load_from_file(self,filepath):\n",
    "    \n",
    "        with open(filepath, 'r') as myfile: # 'p' is the dirpath and 'f' is the filename from the created 'd' dictionary\n",
    "            cell_data=myfile.read()\n",
    "            self.cell_data = json.loads(cell_data)\n",
    "        \n",
    "        for dtype in self.cell_data['base_segments']:\n",
    "            self.cell_data['base_segments'][dtype] = set(self.cell_data['base_segments'][dtype])\n",
    "    \n",
    "        self.cell_data['removed_base_segs'] = set(self.cell_data['removed_base_segs'])\n",
    "        \n",
    "        main_base_id = self.cell_data['metadata']['main_seg']['base']\n",
    "        print(f'Loading a CREST instance for LOADED Reconstruction of {main_base_id}')\n",
    "        \n",
    "        self.load_graph_from_celldata()\n",
    "        self.resolving_seg_overlap()\n",
    "        # self.adjust_annotations_structures()\n",
    "        \n",
    "\n",
    "    def get_addresses(self, required_addresses):\n",
    "        \n",
    "        '''\n",
    "        req_addresses = ['agglo_address', 'base_address', 'em_address', 'cloud_storage_address']\n",
    "        '''\n",
    "        a = ', '.join(required_addresses)\n",
    "\n",
    "        self.db_cursors.execute(f'''SELECT {a} FROM addresses_table LIMIT 1''')\n",
    "\n",
    "        results = self.db_cursors.fetchall()[0]\n",
    "\n",
    "        return results\n",
    "        \n",
    "    def connect_db(self, db_path):\n",
    "\n",
    "        self.db_cursors = sqlite3_connect(db_path, check_same_thread=False).cursor()\n",
    "        \n",
    "    def update_msg(self, msg, layer='status'):\n",
    "        \n",
    "        with self.viewer.config_state.txn() as s:\n",
    "            s.status_messages[layer] = msg\n",
    "\n",
    "    def get_vx_sizes(self):\n",
    "               \n",
    "        self.db_cursors.execute('SELECT * FROM voxel_sizes_table')\n",
    "\n",
    "        self.vx_sizes = {}\n",
    "\n",
    "        for dtype, x, y, z, x_size, y_size, z_size in self.db_cursors.fetchall():\n",
    "\n",
    "            self.vx_sizes[dtype] = [x, y, z]\n",
    "\n",
    "            if dtype == 'em':\n",
    "                self.starting_location = [int(x_size/2), int(y_size/2), int(z_size/2),]\n",
    "\n",
    "    def set_base_seg_merger_layer(self):\n",
    "\n",
    "        self.point_types.append('Base Segment Merger')\n",
    "\n",
    "        with self.viewer.txn(overwrite=True) as s:\n",
    "\n",
    "            s.layers['Base Segment Merger'] = neuroglancer.AnnotationLayer()\n",
    "            s.layers['Base Segment Merger'].filterBySegmentation = [\"segments\"]\n",
    "            s.layers['Base Segment Merger'].linkedSegmentationLayer = {\"segments\": 'base_segs'}\n",
    "            s.layers['Base Segment Merger'].annotationColor = '#ffa500'\n",
    "            s.layers['Base Segment Merger'].tool = \"annotatePoint\"\n",
    "\n",
    "            for pos, point in enumerate(self.cell_data['base_seg_merge_points']):\n",
    "\n",
    "                point_array = array([int(point[x]/self.vx_sizes['em'][x]) for x in range(3)])\n",
    "                pa = neuroglancer.PointAnnotation(id=f'bm_{pos}', point = point_array, segments=[[point[3]]])\n",
    "                s.layers['Base Segment Merger'].annotations.append(pa)                \n",
    "\n",
    "    def change_point(self):\n",
    "\n",
    "        if self.point_pos == len(self.point_types)-1:\n",
    "            self.point_pos = 0\n",
    "        else:\n",
    "            self.point_pos += 1\n",
    "\n",
    "        selected_layer = self.point_types[self.point_pos]\n",
    "\n",
    "        with self.viewer.txn(overwrite=True) as s:\n",
    "            s.selectedLayer.layer = selected_layer\n",
    "            s.selected_layer.visible = True\n",
    "            s.layers[selected_layer].tab = 'Annotations'\n",
    "        \n",
    "        self.update_msg(f'Current Point Annotation Type (P): {selected_layer}', layer='current point type')\n",
    "\n",
    "    def change_cell_structure(self):\n",
    "     \n",
    "        if self.cell_structure_pos == len(self.cell_structures)-1:\n",
    "            self.cell_structure_pos = 0\n",
    "\n",
    "        else:\n",
    "            self.cell_structure_pos += 1\n",
    "            \n",
    "        self.update_msg(f'Current Cell Structure (C): {self.cell_structures[self.cell_structure_pos]}', layer='Current Cell Structure')\n",
    "            \n",
    "    def adjust_annotations_structures(self):\n",
    "        '''\n",
    "        when loading a cell from a file, add necessary annotations and structures if do not exist\n",
    "        '''     \n",
    "        \n",
    "        for p in self.point_types:\n",
    "            if p not in self.cell_data['end_points']:\n",
    "                self.cell_data['end_points'][p] = [] # create the entry for that annotation point\n",
    "        self.point_types = list(set(self.point_types + list(self.cell_data['end_points'].keys())))\n",
    "        self.point_types = [x for x in self.point_types if not ('base' in x.lower() and 'merge' in x.lower())]\n",
    "        \n",
    "        self.set_endpoint_annotation_layers() # reset annotations layers to include any adjustments\n",
    "\n",
    "        existing_struc = [x for x in self.cell_data['base_segments'].keys() if x!= 'unknown']\n",
    "        for dtype in self.cell_structures:\n",
    "            if dtype not in self.cell_data['base_segments'].keys():\n",
    "                self.cell_data['base_segments'][dtype] = set() # create the entry for that annotation point\n",
    "        self.cell_structures = list(set(self.cell_structures) | set(existing_struc))\n",
    "        \n",
    "        self.set_seg_colours() # reset colors for segments\n",
    "        \n",
    "    def set_endpoint_annotation_layers(self): \n",
    "\n",
    "        self.point_types = list(set(self.point_types + list(self.cell_data['end_points'].keys())))\n",
    "        self.point_types = [x for x in self.point_types if not ('base' in x.lower() and 'merge' in x.lower())]\n",
    "\n",
    "        with self.viewer.txn(overwrite=True) as s:\n",
    "\n",
    "            for point_type in self.point_types:\n",
    "\n",
    "                s.layers[point_type] = neuroglancer.AnnotationLayer()\n",
    "                \n",
    "                if point_type == 'post-synaptic':\n",
    "                    s.layers[point_type].annotationColor = '#ff00ff'\n",
    "                    s.layers[point_type].linkedSegmentationLayer = {\"segments\": 'base_segs'} # set it up linked to base_segs\n",
    "                elif point_type == 'pre-synaptic':\n",
    "                    s.layers[point_type].annotationColor = '#00EEEE'\n",
    "                    s.layers[point_type].linkedSegmentationLayer = {\"segments\": 'base_segs'} # set it up linked to base_segs\n",
    "                elif (point_type == 'natural end') | (point_type == 'exit volume'):\n",
    "                    s.layers[point_type].annotationColor = '#FFFF00'\n",
    "                elif point_type == 'uncertain':\n",
    "                    s.layers[point_type].annotationColor = '#EE0000'\n",
    "                else:\n",
    "                    s.layers[point_type].annotationColor = '#ffffff'\n",
    "\n",
    "                s.layers[point_type].tool = \"annotatePoint\"\n",
    "                s.layers[point_type].tab = 'Annotations'\n",
    "\n",
    "                # If data already exists for this point type:\n",
    "\n",
    "        self.load_annotation_layer_points()\n",
    "        \n",
    "    def load_annotation_layer_points(self):\n",
    "        \n",
    "        with self.viewer.txn(overwrite=True) as s:\n",
    "            for point_type in self.point_types:\n",
    "\n",
    "                # If data already exists for this point type:\n",
    "                if point_type in self.cell_data['end_points'].keys():\n",
    "\n",
    "                    for pos, point in enumerate(self.cell_data['end_points'][point_type]):\n",
    "\n",
    "                        if len(point)==3: # then there is no segment ID associated with the annotation point\n",
    "                        \n",
    "                            point_array = array([int(point[x]/self.vx_sizes['em'][x]) for x in range(3)])\n",
    "                            point_id = f'{point_type}_{pos}'\n",
    "                            pa = neuroglancer.PointAnnotation(id=point_id, point = point_array)\n",
    "                            s.layers[point_type].annotations.append(pa)\n",
    "\n",
    "                        if len(point)==4: # then include the segment ID with the annotation point\n",
    "                            point_array = array([int(point[x]/self.vx_sizes['em'][x]) for x in range(3)])\n",
    "                            point_id = f'{point_type}_{pos}'\n",
    "                            segment_id = point[3]\n",
    "                            pa = neuroglancer.PointAnnotation(id=point_id, point = point_array, segments = [[segment_id]])\n",
    "                            s.layers[point_type].annotations.append(pa)                     \n",
    "\n",
    "    def save_point_types_successfully(self):\n",
    "\n",
    "        for t in self.point_types:\n",
    "\n",
    "            this_type_points = []\n",
    "     \n",
    "            for x in self.viewer.state.layers[t].annotations:\n",
    "                if t == 'Base Segment Merger' and x.segments == None:\n",
    "                    c = [int(y) for y in x.point]\n",
    "                    self.update_mtab(f'Error, no segment for point {c}, for point layer {t}, correct and re-save', 'Cell Reconstruction')\n",
    "                    return False\n",
    "\n",
    "                else:\n",
    "                    co_ords = [float(x) for x in list(x.point)]\n",
    "                    co_ords_and_id = [co_ords[x]*self.vx_sizes['em'][x] for x in range(3)]\n",
    "\n",
    "                    if x.segments != None:\n",
    "                        if len(x.segments[0]) > 0:\n",
    "                            co_ords_and_id.append(str(x.segments[0][0]))\n",
    "\n",
    "                    this_type_points.append(co_ords_and_id)\n",
    "\n",
    "            if t == 'Base Segment Merger':\n",
    "                self.cell_data['base_seg_merge_points'] = this_type_points\n",
    "            else:\n",
    "                self.cell_data['end_points'][t] = this_type_points\n",
    "\n",
    "        return True                            \n",
    "\n",
    "    def set_seg_colours(self):\n",
    "        chosen_col = '#D2B48C'\n",
    "        self.chosen_seg_colours = {'unknown': '#D2B48C'} # tan\n",
    "\n",
    "        # acceptable_colours = set(['#FFFF00', '#800080', '#008000', '#FF00FF', '#00FF00', '#FF69B4', '#FF8C00'])\n",
    "        # used_colours = set()\n",
    "\n",
    "        for x in self.cell_structures:\n",
    "\n",
    "            # available_colours = acceptable_colours - used_colours\n",
    "\n",
    "            # if len(available_colours) == 0:\n",
    "            #     available_colours = acceptable_colours\n",
    "            \n",
    "            if x=='multiple':\n",
    "                chosen_col = '#9C661F' # white\n",
    "\n",
    "            if x=='axon':\n",
    "                chosen_col = '#008000' # green\n",
    "\n",
    "            if x=='dendrite':\n",
    "                chosen_col = '#FFFF00' # yellow\n",
    "\n",
    "            if x=='basal dendrite': # orange-red\n",
    "                chosen_col = '#CD4B00'\n",
    "\n",
    "            if x=='apical dendrite': # orange\n",
    "                chosen_col = '#FF8000'\n",
    "    \n",
    "            if x not in self.cell_data['base_segments'].keys():\n",
    "                chosen_col = '#708090' # if not one of the explicitly chosen structures, make it slate gray\n",
    "\n",
    "            # used_colours.add(chosen_col)\n",
    "            self.chosen_seg_colours[x] = chosen_col\n",
    "    \n",
    "    def import_base_segments(self,base_segments):\n",
    "        \n",
    "        '''\n",
    "        base_segments is the list of segments from neuroglancer json (which is why get put in \"unknown\")\n",
    "        '''\n",
    "        # Turn lists back to sets:\n",
    "        self.cell_data['base_segments']['unknown'] = set([str(x) for x in base_segments])\n",
    "                \n",
    "    \n",
    "    def create_pr_graph(self):\n",
    "\n",
    "        seg_id = self.cell_data['metadata']['main_seg']['base']\n",
    "\n",
    "        print(f'Creating base segment graph for cell {seg_id}', 'Cell Reconstruction')\n",
    "\n",
    "        all_base_segs = [str(a) for b in self.cell_data['base_segments'].values() for a in b]\n",
    "        \n",
    "        self.update_base_locations(all_base_segs)\n",
    "\n",
    "              \n",
    "        ####\n",
    "        # Correct base segment locations that got left out \n",
    "        '''td:\n",
    "        figure out why they are like this.\n",
    "        for example, for one the segment id returned '0' even though there was a location returned\n",
    "        '''\n",
    "        no_loc_base_segs = [str(x) for x in all_base_segs if x not in self.cell_data['base_locations']]\n",
    "        if no_loc_base_segs != []:\n",
    "            for s in no_loc_base_segs:\n",
    "                try:\n",
    "                    results_dict = self.get_locations_from_base_segs(s)\n",
    "                    k = list(results_dict.keys())[0] # get key for this segment ID in queried segment location\n",
    "                    self.cell_data['base_locations'][s] = self.get_corrected_xyz(results_dict[k], 'seg') # manually log its location with given segment ID\n",
    "                except: \n",
    "                    print(f'{s} actually no base segment location in SQL... will attach without location later')\n",
    "                    continue\n",
    "            self.cell_data['no_loc_base_segs'] = no_loc_base_segs # did not add this until after some conversions already done\n",
    "        ####\n",
    "        \n",
    "        \n",
    "        print(f'all base locations for {len(all_base_segs)} obtained from SQL database')\n",
    "        \n",
    "        possible_edges = []\n",
    "        agglo_segs_done = set()\n",
    "        base_segs_done = set()\n",
    "\n",
    "        for base_seg in all_base_segs:\n",
    "\n",
    "            if base_seg in base_segs_done: continue # if the base segment has been included, go to next\n",
    "            # if the base segment has not been included yet,\n",
    "\n",
    "            agglo_seg = self.get_agglo_seg_of_base_seg(base_seg) # get its agglomeration segment\n",
    "            children_base_segs = self.get_base_segs_of_agglo_seg(agglo_seg) # and all of the other base segments also in that agglo seg\n",
    "            base_segs_done.update(children_base_segs) # mark all of these base segments as included\n",
    "\n",
    "            if not agglo_seg in agglo_segs_done: # connect the agglomeration segment for those bases if it has not already been done\n",
    "\n",
    "                edges = self.get_edges_from_agglo_seg(agglo_seg)\n",
    "\n",
    "                agglo_segs_done.add(agglo_seg)\n",
    "                possible_edges.extend(edges)\n",
    "\n",
    "        all_bs_set = set(all_base_segs)\n",
    "        possible_edges = [x for x in possible_edges if x[0] in all_bs_set] # only include edges between connect base segments in the reconstruction\n",
    "            # note that this is what probably creates so many disconnected clusters... if a base segment is missing from the agglo, \n",
    "            #then the edges from that segment will not be included and will need to be connected manually\n",
    "        chosen_edges = [x for x in possible_edges if x[1] in all_bs_set]\n",
    "\n",
    "        self.pr_graph = ig_Graph(directed=False)\n",
    "        self.pr_graph.add_vertices(all_base_segs)\n",
    "        self.pr_graph.add_edges(chosen_edges)\n",
    "\n",
    "        print('graph created among all_base_segs')\n",
    "        \n",
    "\n",
    "        self.add_cc_bridging_edges_pairwise()\n",
    "        print('weak clusters connected')\n",
    "        \n",
    "        self.attach_noloc_segs()\n",
    "        print('segments without a location connected')\n",
    "        \n",
    "        self.cell_data['graph_nodes'] = [x['name'] for x in self.pr_graph.vs]\n",
    "        self.cell_data['graph_edges'] = [(self.pr_graph.vs[x.source]['name'], self.pr_graph.vs[x.target]['name']) for x in self.pr_graph.es]\n",
    "\n",
    "\n",
    "        '''\n",
    "        # removed assertion of pr_graph.clusters==1 because importing from a neuroglancer json might \"break\" this and it is ok...\n",
    "        \n",
    "        assert len(self.pr_graph.clusters(mode='weak')) == 1\n",
    "        '''\n",
    "        \n",
    "        n_clusters = len(self.pr_graph.connected_components(mode='weak'))\n",
    "        \n",
    "        print(f'{n_clusters} clusters in graph (note should/would be only 1 if loaded base ID from agglomo fresh)')\n",
    "        \n",
    "        # self.assert_segs_in_sync()\n",
    "        \n",
    "        # print(f'successful assertion that graph segments and segments listed in base_segments match')\n",
    "        \n",
    "    def load_graph_from_celldata(self):\n",
    "\n",
    "        self.pr_graph = ig_Graph()\n",
    "        self.pr_graph.add_vertices(self.cell_data['graph_nodes'])\n",
    "        self.pr_graph.add_edges(self.cell_data['graph_edges'])\n",
    "        \n",
    "    def save_cell_graph(self, directory_path = None, file_name=None, save_to_cloud=False):\n",
    "        \n",
    "        timestamp = str(datetime.now())[:-7].replace(':','.')\n",
    "        main_base_id = self.cell_data['metadata']['main_seg']['base']\n",
    "                \n",
    "        cell_data = deepcopy(self.cell_data)\n",
    "\n",
    "        # Convert sets to lists for saving in json file:\n",
    "        for dtype in cell_data['base_segments'].keys():\n",
    "            cell_data['base_segments'][dtype] = list(cell_data['base_segments'][dtype])\n",
    "        \n",
    "        cell_data['removed_base_segs'] = list(cell_data['removed_base_segs'])\n",
    "\n",
    "        completion_list = list(set(cell_data['metadata']['completion']))\n",
    "        completion_list.sort()\n",
    "        completion_string = ','.join(completion_list).replace('_', ' ')\n",
    "            \n",
    "        cell_data['metadata']['data_sources']['agglo'] = self.agglo_seg\n",
    "        \n",
    "        if directory_path==None:\n",
    "            directory_path = self.save_dir\n",
    "            \n",
    "        if file_name == None:\n",
    "            file_name = f'cell_graph_{main_base_id}_{completion_string}_{timestamp}.json'\n",
    "            \n",
    "#         with open(f'{self.save_dir}/{file_name}', 'w') as fp:\n",
    "#             json_dump(cell_data, fp)\n",
    "        with open(directory_path / file_name, 'w') as fp:\n",
    "            json.dump(cell_data, fp, indent=4)\n",
    "\n",
    "        print(f'Saved cell {main_base_id} reconstruction locally at {timestamp}')\n",
    "\n",
    "    def update_base_locations(self, seg_list):\n",
    "\n",
    "        seg_list = [x for x in seg_list if x not in self.cell_data['base_locations'].keys()]\n",
    "\n",
    "        result_dict = self.get_locations_from_base_segs(seg_list)\n",
    "\n",
    "        for r in result_dict:\n",
    "            self.cell_data['base_locations'][r] = self.get_corrected_xyz(result_dict[r], 'seg')\n",
    "\n",
    "    def get_locations_from_base_segs(self, base_segs, batch_size = 1000):\n",
    "\n",
    "        results = {}\n",
    "\n",
    "        if len(base_segs) > 0:\n",
    "        \n",
    "            num_batches = int(len(base_segs)/batch_size)\n",
    "            \n",
    "            for batch in range(num_batches+1):\n",
    "\n",
    "                q = ','.join([str(x) for x in base_segs[batch*batch_size:(batch+1)*batch_size]])\n",
    "                \n",
    "                query = f\"\"\"SELECT seg_id, x, y, z FROM base_location WHERE seg_id IN ({q})\"\"\"\n",
    "\n",
    "                self.db_cursors.execute(query)\n",
    "\n",
    "                this_batch = {str(x[0]): (int(x[1]), int(x[2]), int(x[3])) for x in self.db_cursors.fetchall()}\n",
    "\n",
    "                results.update(this_batch)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def get_corrected_xyz(self, xyz, adj_key, rel_to_em=False):\n",
    "\n",
    "        result = []\n",
    "\n",
    "        for pos, coord in enumerate(xyz):\n",
    "            result.append(coord*self.vx_sizes[adj_key][pos])\n",
    "            \n",
    "        if rel_to_em==True:\n",
    "            result = [int(result[x]/self.vx_sizes['em'][x]) for x in range(3)]\n",
    "\n",
    "        return result\n",
    "\n",
    "    def get_agglo_seg_of_base_seg(self, base_seg):\n",
    "\n",
    "        base_seg = str(base_seg)\n",
    "\n",
    "        query = f\"\"\"SELECT agglo_id FROM agglo_base_resolved WHERE base_id = {base_seg}\"\"\"\n",
    "\n",
    "        self.db_cursors.execute(query)\n",
    "        agglo_segs = [str(x[0]) for x in self.db_cursors.fetchall()]\n",
    "\n",
    "        assert len(agglo_segs) <= 1\n",
    "\n",
    "        if agglo_segs == []:\n",
    "            return base_seg\n",
    "        else:\n",
    "            return agglo_segs[0]\n",
    "\n",
    "    def get_base_segs_of_agglo_seg(self, agglo_seg):\n",
    "\n",
    "        agglo_seg = str(agglo_seg)\n",
    "\n",
    "        query = f\"\"\"SELECT base_id FROM agglo_base_resolved WHERE agglo_id = {agglo_seg}\"\"\"\n",
    "\n",
    "        self.db_cursors.execute(query)\n",
    "        base_segs = [str(x[0]) for x in self.db_cursors.fetchall()]\n",
    "        base_segs.append(agglo_seg)\n",
    "\n",
    "        return base_segs\n",
    "\n",
    "    def get_edges_from_agglo_seg(self, agglo_seg):\n",
    "\n",
    "        agglo_seg = str(agglo_seg)\n",
    "\n",
    "        query = f\"\"\"SELECT label_a, label_b FROM agglo_to_edges WHERE agglo_id = {agglo_seg}\"\"\"\n",
    "\n",
    "        self.db_cursors.execute(query)\n",
    "        edges = [(str(x[0]), str(x[1])) for x in self.db_cursors.fetchall()]\n",
    "\n",
    "        return edges\n",
    "    \n",
    "    def add_cc_bridging_edges_pairwise(self):\n",
    "        \n",
    "        '''\n",
    "        con_comms = \"connected components\" abbreviation\n",
    "        '''\n",
    "\n",
    "        con_comms = list(self.pr_graph.connected_components(mode='weak'))\n",
    "        print(f'{len(con_comms)} clusters of connected components. Connecting these clusters with nearest base segments.')\n",
    "        while len(con_comms) > 1:\n",
    "\n",
    "            candidate_edges = []\n",
    "\n",
    "            for cc1, cc2 in combinations(con_comms, 2): # gets all possible pairwise combinations between segments\n",
    "                \n",
    "                # get the name of each base segment\n",
    "                cc1_base_segs = [self.pr_graph.vs[x]['name'] for x in cc1]\n",
    "                cc2_base_segs = [self.pr_graph.vs[x]['name'] for x in cc2]\n",
    "\n",
    "                cc1_list = [x for x in cc1_base_segs if x in self.cell_data['base_locations']]\n",
    "                cc2_list = [x for x in cc2_base_segs if x in self.cell_data['base_locations']]\n",
    "\n",
    "                if cc1_list == [] or cc2_list == []:\n",
    "                    continue\n",
    "\n",
    "                sel_cc1, sel_cc2, dist = self.get_closest_dist_between_ccs(cc1_list, cc2_list)\n",
    "                candidate_edges.append([sel_cc1, sel_cc2, dist])\n",
    "\n",
    "            if candidate_edges == []: \n",
    "                return\n",
    "\n",
    "            origin, target, dist = min(candidate_edges, key = lambda x: x[2])\n",
    "\n",
    "            self.pr_graph.add_edges([(origin, target)])\n",
    "            self.cell_data['added_graph_edges_pre_proofreading'].append([origin, target, dist])\n",
    "#             self.update_mtab(f'Added an edge between segments {origin} and {target}, {dist} nm apart', 'Cell Reconstruction')\n",
    "\n",
    "            con_comms = list(self.pr_graph.connected_components(mode='weak'))\n",
    "\n",
    "    def get_closest_dist_between_ccs(self, cc1_node_list, cc2_node_list):\n",
    "\n",
    "        cc1_node_locs = [self.cell_data['base_locations'][x] for x in cc1_node_list]\n",
    "        cc2_node_locs = [self.cell_data['base_locations'][x] for x in cc2_node_list]\n",
    "\n",
    "        f = cdist(cc1_node_locs, cc2_node_locs, 'euclidean')\n",
    "\n",
    "        min_indices = unravel_index(argmin(f, axis=None), f.shape)\n",
    "\n",
    "        sel_cc1 = cc1_node_list[min_indices[0]]\n",
    "        sel_cc2 = cc2_node_list[min_indices[1]]\n",
    "        dist = int(f[min_indices])  \n",
    "\n",
    "        return sel_cc1, sel_cc2, dist\n",
    "            \n",
    "    def attach_noloc_segs(self):\n",
    "        ''' NOTE that this does not run (it returns) if self.pr_graph.clusters(mode='weak') == 1\n",
    "        This is a case that is asserted in oringinal CREST.py in '''\n",
    "        \n",
    "        # For isolated segments without locations, attach to largest connected component:\n",
    "        remaining_cc = list(self.pr_graph.connected_components(mode='weak'))\n",
    "\n",
    "        if len(remaining_cc) == 1: return\n",
    "\n",
    "        if len(remaining_cc) > 1:\n",
    "            no_loc_base_segs = set([x['name'] for x in self.pr_graph.vs if x['name'] not in self.cell_data['base_locations']])\n",
    "            largest_cc = max(remaining_cc, key = lambda x: len(x))\n",
    "\n",
    "            '''\n",
    "            #### Raises TypeError: unsupported operand type(s) for &: 'list' and 'set'\n",
    "            for cc in remaining_cc:\n",
    "                no_loc_this_cc = cc & no_loc_base_segs\n",
    "                if cc != largest_cc and no_loc_this_cc != set():\n",
    "                    rand_seg1 = random_choice(list(no_loc_this_cc))\n",
    "                    rand_seg2 = random_choice(list(largest_cc))\n",
    "                    self.pr_graph.add_edges([(rand_seg1, rand_seg2)])\n",
    "                    self.cell_data['added_graph_edges_pre_proofreading'].append([rand_seg1, rand_seg2, 'unknown'])\n",
    "#                     print(f'Added an edge between segments {rand_seg1} and {rand_seg2}', 'Cell Reconstruction')\n",
    "            '''\n",
    "            # I think the following block replaces the commented out above with the correct intension?\n",
    "            nodes_names = [x['name'] for x in self.pr_graph.vs]\n",
    "            for cc in remaining_cc:\n",
    "                # no_loc_this_cc = cc & no_loc_base_segs # raises TypeError: unsupported operand type(s) for &: 'list' and 'set'\n",
    "                no_loc_this_cc = set([nodes_names[i] for i in remaining_cc[1]])& set(no_loc_base_segs) # I think this is what Alex was going for?\n",
    "                if cc != largest_cc and no_loc_this_cc != set():\n",
    "                    rand_seg1 = random_choice(list(no_loc_this_cc))\n",
    "                    rand_seg2 = random_choice(list(largest_cc))\n",
    "                    self.pr_graph.add_edges([(rand_seg1, rand_seg2)])\n",
    "                    self.cell_data['added_graph_edges_pre_proofreading'].append([rand_seg1, rand_seg2, 'unknown'])\n",
    "            #                     print(f'Added an edge between segments {rand_seg1} and {rand_seg2}', 'Cell Reconstruction')\n",
    "\n",
    "\n",
    "    def assert_segs_in_sync(self, return_segs=False):\n",
    "        \n",
    "        graph_segs = set([x['name'] for x in self.pr_graph.vs])\n",
    "        listed_segs = set([a for b in [self.cell_data['base_segments'][cs] for cs in self.cell_data['base_segments'].keys()] for a in b])\n",
    "\n",
    "        assert listed_segs == graph_segs\n",
    "\n",
    "        if self.launch_viewer==True:\n",
    "            displayed_segs = set([str(x) for x in self.viewer.state.layers['base_segs'].segments]) # not connected to neuroglancer, so not relevant\n",
    "\n",
    "            if not displayed_segs == graph_segs:\n",
    "                self.update_displayed_segs()\n",
    "        \n",
    "        if return_segs:\n",
    "#             return displayed_segs\n",
    "            return graph_segs\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def get_ds_segs_of_certain_col(self, base_seg, colour):\n",
    "\n",
    "        ds = self.get_downstream_base_segs(base_seg)[0]\n",
    "\n",
    "        # If any of the downstream segments doesn't have a colour, set it to tan:\n",
    "        with self.viewer.txn(overwrite=True) as s:\n",
    "\n",
    "            for bs in ds:\n",
    "                if int(bs) not in s.layers['base_segs'].segment_colors.keys():\n",
    "                    s.layers['base_segs'].segment_colors[int(bs)] = '#D2B48C'\n",
    "\n",
    "            ds = set([x for x in ds if s.layers['base_segs'].segment_colors[int(x)] == colour])\n",
    "        \n",
    "        return ds\n",
    "\n",
    "    def update_displayed_segs(self):\n",
    "\n",
    "        displayed_segs = set([str(x) for x in self.viewer.state.layers['base_segs'].segments])\n",
    "        listed_segs = set([x for y in self.cell_data['base_segments'].values() for x in y])\n",
    "        graph_segs = set([x['name'] for x in self.pr_graph.vs])\n",
    "\n",
    "        assert listed_segs == graph_segs\n",
    "\n",
    "        # Identify segments that failed to be removed from the viewer:\n",
    "        segs_to_remove = displayed_segs - listed_segs\n",
    "\n",
    "        # Identify segments that failed to be added to the viewer:\n",
    "        missing_segs = listed_segs - displayed_segs\n",
    "        # missing_focus_segs = self.focus_seg_set - set([str(x) for x in self.viewer.state.layers['focus_segs'].segments])\n",
    "\n",
    "        if not missing_segs == set():\n",
    "            # Correct the viewer:\n",
    "            with self.viewer.txn(overwrite=True) as s:\n",
    "                \n",
    "                layer = 'base_segs'\n",
    "            \n",
    "                for bs in missing_segs:\n",
    "                    s.layers[layer].segment_colors[int(bs)] = '#D2B48C'\n",
    "                    s.layers[layer].segments.add(int(bs)) \n",
    "\n",
    "                for bs in segs_to_remove:\n",
    "                    if int(bs) in s.layers[layer].segments:\n",
    "                        s.layers[layer].segments.remove(int(bs))\n",
    "                            \n",
    "    def add_or_remove_seg(self, action_state):  \n",
    "\n",
    "        rel_layer = 'base_segs'\n",
    "        \n",
    "        base_seg = self.check_selected_segment(rel_layer, action_state, banned_segs = [self.cell_data['anchor_seg']])\n",
    "\n",
    "        if base_seg == 'None': return\n",
    "\n",
    "        # Otherwise, add or remove to main layer:\n",
    "        \n",
    "        displayed_segs = self.assert_segs_in_sync(return_segs=True)\n",
    "\n",
    "        if base_seg in displayed_segs:\n",
    "\n",
    "            self.remove_downstream_base_segs(base_seg)\n",
    "\n",
    "        \n",
    "        else:\n",
    "\n",
    "            # Adding a segment:\n",
    "\n",
    "            agglo_seg = self.check_selected_segment('agglo', action_state)\n",
    "  \n",
    "            if agglo_seg == 'None': return\n",
    "\n",
    "            constituent_base_ids = self.get_base_segs_of_agglo_seg(agglo_seg)\n",
    "            print(f'{len(constituent_base_ids)} other base segments in the agglo segment; max number can add is {self.max_num_base_added}')\n",
    "\n",
    "            if len(constituent_base_ids) > self.max_num_base_added:\n",
    "                base_ids = [base_seg]\n",
    "                #self.large_agglo_segs.add(agglo_seg)\n",
    "            else:\n",
    "                base_ids = constituent_base_ids\n",
    "\n",
    "            current_segs = self.assert_segs_in_sync(return_segs=True)\n",
    "\n",
    "            num_base_segs_this_agglo_seg = len(base_ids)\n",
    "            base_ids = [x for x in base_ids if x not in current_segs]\n",
    "            num_base_segs_not_already_included = len(base_ids)\n",
    "\n",
    "            if num_base_segs_this_agglo_seg > num_base_segs_not_already_included:\n",
    "\n",
    "                base_ids = [x for x in base_ids if x not in self.cell_data['removed_base_segs']]\n",
    "\n",
    "                if not base_seg in base_ids:\n",
    "                    base_ids.append(base_seg)\n",
    "    \n",
    "            self.update_base_locations(base_ids)\n",
    "            self.pr_graph.add_vertices(base_ids)\n",
    "\n",
    "            if len(base_ids) > 1:\n",
    "                edges = self.get_edges_from_agglo_seg(agglo_seg)\n",
    "                edges = [x for x in edges if (x[0] in base_ids and x[1] in base_ids)]\n",
    "                self.pr_graph.add_edges(edges)\n",
    "\n",
    "            join_msg = self.add_closest_edge_to_graph(base_ids, base_seg) \n",
    "\n",
    "            # Update lists of base segments and displayed segs:\n",
    "            self.cell_data['base_segments']['unknown'].update(set(base_ids))\n",
    "\n",
    "\n",
    "            with self.viewer.txn(overwrite=True) as s:\n",
    "\n",
    "                for bs in base_ids:\n",
    "                    s.layers['base_segs'].segment_colors[int(bs)] = '#D2B48C'\n",
    "                    s.layers['base_segs'].segments.add(int(bs))\n",
    "\n",
    "\n",
    "            self.update_displayed_segs() \n",
    "            self.assert_segs_in_sync()\n",
    "\n",
    "            print(f'Added {len(base_ids)} base segments from agglomerated segment {agglo_seg}{join_msg}')\n",
    "\n",
    "    def remove_downstream_base_segs(self, base_seg):\n",
    "\n",
    "        segs_to_remove, n_con_com = self.get_downstream_base_segs(base_seg)\n",
    "\n",
    "        self.assert_segs_in_sync()\n",
    "\n",
    "        # Remove from lists and segmentation layer:\n",
    "        for cs in self.cell_data['base_segments'].keys():\n",
    "            self.cell_data['base_segments'][cs] -= set(segs_to_remove)\n",
    "\n",
    "        self.pr_graph.delete_vertices(segs_to_remove)\n",
    "        # self.focus_seg_set -= set(segs_to_remove)\n",
    "\n",
    "        with self.viewer.txn(overwrite=True) as s:\n",
    "\n",
    "            for bs in segs_to_remove:\n",
    "\n",
    "                if int(bs) in s.layers['base_segs'].segments:\n",
    "                    s.layers['base_segs'].segments.remove(int(bs))\n",
    "\n",
    "        self.assert_segs_in_sync()\n",
    "            \n",
    "        self.cell_data['removed_base_segs'].update(set(segs_to_remove))\n",
    "        print(f'{len(segs_to_remove)} base segments removed from {n_con_com} connected components')\n",
    "        self.cell_data['added_graph_edges'] = [x for x in self.cell_data['added_graph_edges'] if (x[0] not in segs_to_remove) and (x[1] not in segs_to_remove)]\n",
    "\n",
    "    def get_downstream_base_segs(self, base_seg):\n",
    "\n",
    "        edge_backup = [(self.pr_graph.vs[p_ix]['name'], base_seg) for p_ix in self.pr_graph.neighbors(base_seg)]\n",
    "\n",
    "        self.pr_graph.delete_vertices([base_seg])\n",
    "\n",
    "        current_cc = list(self.pr_graph.connected_components(mode='weak'))\n",
    "        current_cc_seg_ids = [[self.pr_graph.vs[i]['name'] for i in c] for c in current_cc]\n",
    "        ccs_to_remove = [cc for cc in current_cc_seg_ids if self.cell_data['anchor_seg'] not in cc]\n",
    "        segs_to_remove = [str(x) for y in ccs_to_remove for x in y if str(x) != '0']\n",
    "        segs_to_remove.append(base_seg)\n",
    "\n",
    "        self.pr_graph.add_vertices([base_seg])\n",
    "        self.pr_graph.add_edges(edge_backup)\n",
    "\n",
    "        return segs_to_remove, len(current_cc)\n",
    "    \n",
    "    def add_closest_edge_to_graph(self, new_segs, seg_to_link):\n",
    "\n",
    "        assert len(self.pr_graph.connected_components(mode='weak')) == 2\n",
    "\n",
    "        # Some segments do not have locations recorded:\n",
    "        current_cell_node_list = [x['name'] for x in self.pr_graph.vs if x['name'] not in new_segs]\n",
    "        current_cell_node_list = [x for x in current_cell_node_list if x in self.cell_data['base_locations']]\n",
    "        \n",
    "        # Then determine new segments that are acceptable as partners\n",
    "        if seg_to_link in self.cell_data['base_locations'].keys():\n",
    "            new_segs = [seg_to_link]\n",
    "        else:\n",
    "            new_segs = [x for x in new_segs if x in self.cell_data['base_locations']]\n",
    "\n",
    "        sel_curr, sel_new, dist = self.get_closest_dist_between_ccs(current_cell_node_list, new_segs)\n",
    "        \n",
    "        self.pr_graph.add_edges([(sel_curr, sel_new)])\n",
    "        self.cell_data['added_graph_edges'].append([sel_curr, sel_new, dist])\n",
    "\n",
    "        assert len(self.pr_graph.connected_components(mode='weak')) == 1     \n",
    "\n",
    "        return f', linked base segments {sel_curr} and {sel_new}, {round(dist)}nm apart, '\n",
    "\n",
    "    def resolving_seg_overlap(self):\n",
    "\n",
    "        for p1, p2 in combinations(self.cell_data['base_segments'].keys(), 2):\n",
    "\n",
    "            common_segments = set(self.cell_data['base_segments'][p1]) & set(self.cell_data['base_segments'][p2])\n",
    "\n",
    "            if common_segments != set():\n",
    "\n",
    "                self.update_mtab(f\"Base segments {common_segments} are present in both {p1} and {p2} layers, moving to 'unknown'\", 'Cell Reconstruction')\n",
    "\n",
    "                for dtype in p1, p2:\n",
    "                    if dtype != 'unknown':\n",
    "                        self.cell_data['base_segments'][dtype] -= common_segments\n",
    "\n",
    "                self.cell_data['base_segments']['unknown'].update(common_segments)\n",
    "\n",
    "    def mark_branch_in_colour(self, action_state):\n",
    "\n",
    "        base_seg = self.check_selected_segment('base_segs', action_state, banned_segs = [self.cell_data['anchor_seg']])\n",
    "\n",
    "        if base_seg == 'None': return\n",
    "\n",
    "        if base_seg not in [x['name'] for x in self.pr_graph.vs]:\n",
    "            print(f'Base segment {base_seg} was not in the base segment graph, updating displayed segments ...')\n",
    "            self.update_displayed_segs()\n",
    "            return\n",
    "\n",
    "        col = self.viewer.state.layers['base_segs'].segment_colors\n",
    "\n",
    "        if int(base_seg) not in col.keys(): return\n",
    "\n",
    "        current_colour = col[int(base_seg)]\n",
    "        downstream_segs = self.get_ds_segs_of_certain_col(base_seg, current_colour)\n",
    "\n",
    "        if current_colour != '#D2B48C':\n",
    "            cell_part = 'unknown'\n",
    "        else:\n",
    "            cell_part = self.cell_structures[self.cell_structure_pos]\n",
    "        \n",
    "        new_colour = self.chosen_seg_colours[cell_part]\n",
    "\n",
    "        for cs in self.cell_data['base_segments'].keys():\n",
    "\n",
    "            if cs == cell_part:\n",
    "                self.cell_data['base_segments'][cs].update(downstream_segs)\n",
    "            else:\n",
    "                self.cell_data['base_segments'][cs] -= downstream_segs\n",
    "\n",
    "        with self.viewer.txn(overwrite=True) as s:\n",
    "            for bs in downstream_segs:\n",
    "                s.layers['base_segs'].segment_colors[int(bs)] = new_colour\n",
    "\n",
    "    def check_selected_segment(self, layer, action, banned_segs = [], acceptable_segs='all'):\n",
    "\n",
    "        if layer not in action.selectedValues: \n",
    "            return 'None'\n",
    "\n",
    "        selected_segment = str(action.selected_values.get(layer).value)\n",
    "        banned_segs.extend(['None', '0'])\n",
    "\n",
    "        if selected_segment in banned_segs:\n",
    "            return 'None'\n",
    "        else:\n",
    "            if acceptable_segs != 'all':\n",
    "                if selected_segment not in acceptable_segs:\n",
    "                    print(f'Segment {selected_segment} not in current graph')\n",
    "                    return 'None'\n",
    "\n",
    "            return selected_segment\n",
    "        \n",
    "    def change_anchor_seg(self, action_state):  \n",
    "\n",
    "        base_seg = self.check_selected_segment('base_segs', action_state, banned_segs=[self.cell_data['anchor_seg']])\n",
    "        if base_seg == 'None': return\n",
    "\n",
    "        with self.viewer.txn(overwrite=True) as s:\n",
    "            s.layers['base_segs'].segment_colors[int(self.cell_data['anchor_seg'])] = '#D2B48C'\n",
    "            s.layers['base_segs'].segment_colors[int(base_seg)] = '#1e90ff'\n",
    "            \n",
    "        self.cell_data['anchor_seg'] = deepcopy(base_seg)\n",
    "        \n",
    "    def change_view(self, location, css=None, ps=None):\n",
    "\n",
    "        with self.viewer.txn(overwrite=True) as s:\n",
    "\n",
    "            dimensions = neuroglancer.CoordinateSpace(\n",
    "                scales=self.vx_sizes['em'],\n",
    "                units='nm',\n",
    "                names=['x', 'y', 'z']   )\n",
    "\n",
    "            s.showSlices = False\n",
    "            s.dimensions = dimensions\n",
    "            s.position = array(location)\n",
    "            s.layout = \"xy-3d\"\n",
    "\n",
    "            if css != None:\n",
    "                s.crossSectionScale = css\n",
    "            \n",
    "            if ps != None:\n",
    "                s.projectionScale = ps\n",
    "                \n",
    "    def reset_seg_pr_layers(self, two_d_intensity = 0.5):\n",
    "\n",
    "        with self.viewer.txn(overwrite=True) as s:\n",
    "\n",
    "            s.layers['em'] = neuroglancer.ImageLayer(source = self.em)\n",
    "\n",
    "            s.layers['agglo'] = neuroglancer.SegmentationLayer(source = self.agglo_seg, segment_colors={})\n",
    "            s.layers['agglo'].pick = False\n",
    "            s.layers['agglo'].visible = True\n",
    "            s.layers['agglo'].ignoreNullVisibleSet = False\n",
    "            s.layers['agglo'].selectedAlpha = two_d_intensity\n",
    "            s.layers['agglo'].objectAlpha = 1.00\n",
    "            \n",
    "            all_segs = [a for b in self.cell_data['base_segments'].values() for a in b]\n",
    "\n",
    "            s.layers['base_segs'] = neuroglancer.SegmentationLayer(source = self.base_seg, segments=all_segs, segment_colors={})\n",
    "            s.layers['base_segs'].ignoreNullVisibleSet = False\n",
    "            s.layers['base_segs'].pick = False\n",
    "            s.layers['base_segs'].selectedAlpha = two_d_intensity #For 2D\n",
    "\n",
    "            for dtype in self.cell_data['base_segments'].keys():\n",
    "\n",
    "                for seg in self.cell_data['base_segments'][dtype]:\n",
    "                    s.layers['base_segs'].segment_colors[int(seg)] = self.chosen_seg_colours[dtype]\n",
    "\n",
    "            s.layers['base_segs'].segment_colors[int(self.cell_data['anchor_seg'])] = '#1e90ff'\n",
    "\n",
    "    def import_annotations(self,neuroglancer_data, neuroglancer_layer_name, crest_layer_name):\n",
    "\n",
    "        for n, c in zip(neuroglancer_layer_name,crest_layer_name):\n",
    "            \n",
    "            # get the 'layers' dictionary that has that name\n",
    "\n",
    "            neuroglancer_layer = next((item for item in neuroglancer_data['layers'] if item[\"name\"] == n), None)\n",
    "\n",
    "            # create the annotation list for CREST and put it into cell_data\n",
    "\n",
    "            annotation_list = []\n",
    "\n",
    "            for v in neuroglancer_layer['annotations']:\n",
    "                # print(v)\n",
    "                corrected_location = self.get_corrected_xyz(v['point'], 'seg')\n",
    "\n",
    "                if 'segments' not in v.keys():\n",
    "                    annotation_list.extend([corrected_location])\n",
    "                if 'segments' in v.keys():\n",
    "                    annotation_list.extend([corrected_location + v['segments'][0]])\n",
    "\n",
    "            self.cell_data['end_points'][c].extend(annotation_list)\n",
    "\n",
    "    def define_ctype(self,ctype, method):\n",
    "        '''\n",
    "        method = \"manual\" or \"auto\" \n",
    "        ctype = \"lg/lf/mg1/mg2/mgx/gc/mli/uk...\"\n",
    "        '''\n",
    "        try:\n",
    "            self.cell_data['metadata']['cell-type'][method] = ctype\n",
    "        except KeyError:\n",
    "            self.cell_data['metadata']['cell-type']={'auto':'','manual':''}\n",
    "            self.cell_data['metadata']['cell-type'][method] = ctype\n",
    "\n",
    "    def get_ctype(self,method):\n",
    "        '''\n",
    "        method = \"manual\" or \"auto\" \n",
    "        '''\n",
    "        ctype = ''\n",
    "        \n",
    "        try:\n",
    "            ctype = self.cell_data['metadata']['cell-type'][method]\n",
    "        except KeyError:\n",
    "            self.cell_data['metadata']['cell-type']={'auto':'','manual':''}\n",
    "\n",
    "        try:\n",
    "            ctype = self.cell_data['metadata']['cell-type'][method]\n",
    "        except Exception:\n",
    "            print('cell type not defined for this cell yet -- use cell_type.define(ctype,method)')\n",
    "\n",
    "        return ctype\n",
    "    \n",
    "def import_settings(dict_json):\n",
    "    with open(dict_json, 'r') as myfile: # 'p' is the dirpath and 'f' is the filename from the created 'd' dictionary\n",
    "        settings_dict=myfile.read()\n",
    "\n",
    "    return settings_dict\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc18eab-9366-478a-bfbc-0e3bb4eef890",
   "metadata": {},
   "source": [
    "# USING THE CREST_JSON class\n",
    "\n",
    "## Settings definitions\n",
    "\n",
    "Whether you are converting from neuroglancer or creating a new reconstruction, the settings_dict parameters is needed to create CREST json files with correct formatting. \n",
    "- 'save_dir' : the directory where JSON files are saved \n",
    "- 'cred' and 'db_path' : specify the path to the agglomeration database file on your local computer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac738a5-547a-40b9-aa5c-a606b15f690a",
   "metadata": {},
   "outputs": [],
   "source": [
    "settings_dict = {\n",
    "    'save_dir' : r\"C:\\Users\\labuser\\Documents\\eCREST-local-files\\in-progress\",\n",
    "    'max_num_base_added' : 1000,\n",
    "    'cell_structures' : ['unknown','axon', 'basal dendrite', 'apical dendrite', 'dendrite', 'multiple'],\n",
    "    'annotation_points' : ['exit volume', 'natural end', 'uncertain', 'pre-synaptic', 'post-synaptic'],\n",
    "    'db_path' : r\"C:\\Users\\labuser\\Documents\\eCREST-local-files\\Mariela_bigquery_exports_agglo_v230111c_16_crest_proofreading_database.db\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e27a53c-cb5c-41e1-a4e1-afed0def0c65",
   "metadata": {},
   "source": [
    "## Proofread using eCREST\n",
    "\n",
    "Initialize with either:\n",
    "- (segment_id, segment_list): the main_base_id from the neuroglancer file you are converting and a list of base_segments.\n",
    "- (segment_id): a \"main_base_id\"\n",
    "- (filepath): an existing CREST json file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c25bfad-c0aa-46b4-821b-44513909fe9f",
   "metadata": {},
   "source": [
    "### 1. Create a crest_json object that launches a proofreading instance of neuroglancer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411ea812-727e-4908-a5d6-a8875b8a0ade",
   "metadata": {},
   "source": [
    "#### NEW reconstruction from segment ID\n",
    "\n",
    "If you wanted to start reconstructing a new cell from a main base segment, \n",
    "you would use the following code block to launch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f035b75-f726-468c-bfeb-807365074e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_id = 558129604\n",
    "crest = ecrest(settings_dict,segment_id = segment_id, launch_viewer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bf264d-62ca-480c-9ed6-c572097c0b0f",
   "metadata": {},
   "source": [
    "#### EDIT reconstruction from file\n",
    "\n",
    "If you wanted to edit a reconstruction from an existing file, \n",
    "you would use the following code block to launch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac08c0d3-5c67-44d1-9ac1-9d1240894cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = Path(settings_dict['save_dir'])\n",
    "filename = 'cell_graph_126649726__2023-03-23 14.21.35.json'\n",
    "\n",
    "crest = ecrest(settings_dict,filepath= json_path / filename, launch_viewer=True)\n",
    "\n",
    "################\n",
    "## You can also open using a full copy/pasted filepath \n",
    "## (instead of directory path and file name... which is better for looping over cells)\n",
    "\n",
    "# filepath = r\"C:\\Users\\PerksLab\\Downloads\\cell_graph_133378529__2023-03-27 14.03.02.json\"\n",
    "# crest = ecrest(settings_dict,filepath = filepath, launch_viewer=True)\n",
    "########################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15bd506-f9b0-47c6-85f3-5027c245f333",
   "metadata": {},
   "source": [
    "### 2. SAVE YOUR WORK BEFORE CLOSING NEUROGLANCER! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dd1e96-0806-494b-a868-50abcd676574",
   "metadata": {},
   "outputs": [],
   "source": [
    "crest.save_cell_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56183ec5-58d7-46c6-adef-06ac37c51901",
   "metadata": {},
   "source": [
    "### 3. CELL TYPING\n",
    "\n",
    "If part of your job as a reconstructor is to identify cell types, then you can use the following blocks of code.\n",
    "First, check if it is already defined (and what the cell type was defined as).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ea5dc6-b9d1-494c-a116-ac568e369a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign which method you are using (manual or auto)\n",
    "method = 'manual'\n",
    "\n",
    "## Do not edit\n",
    "crest.get_ctype(method)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a727f3-6ae1-4b96-9af4-9c2f066e6462",
   "metadata": {},
   "source": [
    "If not defined (or defined incorrectly), then define it.\n",
    "> OPTIONS: mg1, mg2, mgx, lg, lf, lx, mli, gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29285ffc-960f-4c2e-8962-7d2fa40953e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the cell type and which method you are using (manual or auto)\n",
    "cell_type = ''\n",
    "method = 'manual'\n",
    "\n",
    "## Do not edit\n",
    "crest.define_ctype(cell_type,method)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a874946-0275-46e0-aec7-be6ba4271ec5",
   "metadata": {},
   "source": [
    "## Convert From Neuroglancer to eCREST\n",
    "\n",
    "Run the following code cell to convert neuroglancer json files to eCREST json files. \n",
    "\n",
    "Uses \"conversion_specs.json\" to batch process conversion.\n",
    "\n",
    "Conversion using \"conversion_specs.json\" expects:\n",
    "- a folder of neuroglancer json files (with filenames standardized like in Google Drive)\n",
    "- \"dirname\" is the folder containing neuroglancer json files to be converted\n",
    "- that the \"conversion_specs.json\" is in the ```settings_dict['save_dir']``` key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2848ec1f-7cdd-4f7c-93a2-2025c79806ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion_specs_filename = \"conversion_specs_NoSynapses.json\"\n",
    "\n",
    "with open(Path(settings_dict['save_dir']) / conversion_specs_filename) as f:\n",
    "    conversion_specs = json.load(f)\n",
    "\n",
    "p = Path(conversion_specs['dirname'])\n",
    "\n",
    "for cell_id, info in conversion_specs['cell_info'].items():\n",
    "   \n",
    "    f = info['filename']\n",
    "    neuroglancer_layer_name = info['neuroglancer_layer_name']\n",
    "    crest_layer_name = info['crest_layer_name']\n",
    "  \n",
    "    ## Get main_base_seg_ID from filename or from list of segment IDs\n",
    "    main_base_id = f.split('_')[1] # gets the base segment ID from the name\n",
    "    \n",
    "    try:\n",
    "        assert cell_id == main_base_id, f'cell id and filename do not match in conversion json; moving on to next cell without completing this one'\n",
    "    except AssertionError as msg:\n",
    "        print(msg)\n",
    "        #add error message to json\n",
    "        with open(crest.save_dir / conversion_specs_filename, \"r\") as f:\n",
    "            loaded = json.load(f)\n",
    "        loaded['cell_info'][cell_id]['errors'].append(str(msg))\n",
    "        with open(crest.save_dir / conversion_specs_filename, \"w\") as f:\n",
    "            json.dump(loaded, f, indent=4)\n",
    "        continue\n",
    "    \n",
    "    ## Load the neuroglancer json\n",
    "    print(f'you have selected cell {cell_id} to convert')\n",
    "    \n",
    "    with open(p / f, 'r') as myfile: # 'p' is the dirpath and 'f' is the filename from the created 'd' dictionary\n",
    "        neuroglancer_data = json.load(myfile)\n",
    "\n",
    "    print(f'Obtaining base_seg IDs from segmentation layer of neuroglancer json.')\n",
    "\n",
    "    ## Obtain the list of base_segments from the neuroglancer json\n",
    "    segmentation_layer = next((item for item in neuroglancer_data['layers'] if item[\"source\"] == 'brainmaps://10393113184:ell:roi450um_seg32fb16fb_220930'), None)\n",
    "    try:\n",
    "        # add annotation layer\n",
    "        \n",
    "        base_segment_list_ng = segmentation_layer['segments']\n",
    "    except TypeError as msg:\n",
    "        print(msg, f': segmentation layer source is different; moving on to next cell without completing this one')\n",
    "        #add error message to json\n",
    "        with open(crest.save_dir / conversion_specs_filename, \"r\") as f:\n",
    "            loaded = json.load(f)\n",
    "        loaded['cell_info'][cell_id]['errors'].append(str(msg) + f': segmentation layer source is different; moving on to next cell without completing this one')\n",
    "        with open(crest.save_dir / conversion_specs_filename, \"w\") as f:\n",
    "            json.dump(loaded, f, indent=4)\n",
    "        continue\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    print(f'creating a crest_json object with no viewer for this cell')\n",
    "    ## Create CREST instance with no viewer, segment_list, and segment_id\n",
    "    crest = ecrest(settings_dict, segment_id = main_base_id, segment_list = base_segment_list_ng, launch_viewer=False)\n",
    "\n",
    "    print(f'importing annotation layers from neuroglancer')\n",
    "    ## Get annotations from neuroglancer -- iterate through one layer at a time to check for errors in layer names\n",
    "    for nl_, cl_ in zip(neuroglancer_layer_name, crest_layer_name):\n",
    "\n",
    "        # get the 'layers' dictionary that has that name\n",
    "        neuroglancer_layer = next((item for item in neuroglancer_data['layers'] if item[\"name\"] == nl_), None)\n",
    "\n",
    "        if neuroglancer_layer != None:\n",
    "            if cl_ in crest.point_types:\n",
    "                # add annotation layer\n",
    "                crest.import_annotations(neuroglancer_data, [nl_], [cl_])\n",
    "                print(f\"Imported - {nl_} - layer from neuroglancer annotations tabs for cell {crest.cell_data['metadata']['main_seg']['base']} as - {cl_} -.\")\n",
    "            else: \n",
    "                msg = f\"CREST layer name - {cl_} - incorrect for cell {crest.cell_data['metadata']['main_seg']['base']} in conversion_json\"\n",
    "                print(msg)\n",
    "                #add error message to json\n",
    "                with open(crest.save_dir / conversion_specs_filename, \"r\") as f:\n",
    "                    loaded = json.load(f)\n",
    "                loaded['cell_info'][cell_id]['errors'].append(str(msg))\n",
    "                with open(crest.save_dir / conversion_specs_filename, \"w\") as f:\n",
    "                    json.dump(loaded, f, indent=4)\n",
    "        else:\n",
    "            msg = f\"no layer by the name - {nl_} - in neuroglancer json for cell {crest.cell_data['metadata']['main_seg']['base']}\"\n",
    "            print(msg)\n",
    "            #add error message to json\n",
    "            with open(crest.save_dir / conversion_specs_filename, \"r\") as f:\n",
    "                loaded = json.load(f)\n",
    "            loaded['cell_info'][cell_id]['errors'].append(str(msg))\n",
    "            with open(crest.save_dir / conversion_specs_filename, \"w\") as f:\n",
    "                json.dump(loaded, f, indent=4)\n",
    "\n",
    "\n",
    "    ## Save the cell_data as json\n",
    "    print(f'saving cell {cell_id} with completed graph and annotations layers imported')\n",
    "    crest.save_cell_graph() # If do not give file_path, then it will auto-generate one like CREST produces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21516cc1-aa10-4222-b4a1-c96b30ad4777",
   "metadata": {},
   "source": [
    "## Other..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8189f733-6892-4e71-875b-bddfcbaa4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = Path(\"D:\\electric-fish\\eCREST\\CREST_settings.json\")\n",
    "with open(filepath, \"r\") as f:\n",
    "    loaded = json.load(f)\n",
    "\n",
    "with open(filepath, \"w\") as f:\n",
    "    json.dump(loaded, f, indent=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
