{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad326bee-88d1-4c8a-b521-c31f2244dbfb",
   "metadata": {},
   "source": [
    "# Convert json from neuroglancer to CREST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f45b074-5340-41f6-86c2-53f579932600",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73173909-7e4c-4e66-9216-ec31ae11d47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################################################ \n",
    "# Get the latest CREST files for each ID within the target folder (dirname)\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "from sqlite3 import connect as sqlite3_connect\n",
    "from sqlite3 import DatabaseError\n",
    "from igraph import Graph as ig_Graph\n",
    "from igraph import plot as ig_plot\n",
    "from scipy.spatial.distance import cdist\n",
    "from random import choice as random_choice\n",
    "from itertools import combinations\n",
    "from numpy import array, unravel_index, argmin, mean\n",
    "from copy import deepcopy\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774c555f-bd8a-4922-85e1-50aaf8ec727a",
   "metadata": {},
   "source": [
    "### Define a 'crest_json' class using functions from CREST.py\n",
    "\n",
    "An instance of this object will be able to format itself and save itself as a CREST-style .json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "6c77ccc8-da3d-4f40-bee1-8c5d1dcfbd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class crest_json:\n",
    "    \n",
    "    def __init__(self,main_base_id, db_path):\n",
    "        \n",
    "        '''\n",
    "        At some point, store these initialization values (ie addresses, keys lists) as a 'params' file that can be provided to the init function instead of hard-coding\n",
    "        \n",
    "        main_base_id : base segment ID from neuroglancer list\n",
    "        \n",
    "        db_path : filepath to agglomeration database sql file locally on computer\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        print(f'Creating CREST file for {main_base_id}', 'Cell Reconstruction')\n",
    "        \n",
    "        self.db_cursors = {key: {} for key in ['Cell Reconstruction']}\n",
    "        self.db_paths = {key: {} for key in ['Cell Reconstruction']}\n",
    "        self.db_path_labels = {key: {} for key in ['Cell Reconstruction']}\n",
    "        self.settings_dict = {key: {} for key in ['Cell Reconstruction']}\n",
    "        \n",
    "        # Create the connection to the database (right now just for 'Cell Reconstruction')\n",
    "        self.update_selected_db(db_path) \n",
    "        \n",
    "        # Set up addresses\n",
    "        # addresses are stored in the agglomo SQL file\n",
    "        \n",
    "        required_addresses = ['agglo_address', 'base_address', 'em_address', 'cloud_storage_address']\n",
    "        [self.agglo_seg, self.base_seg, self.em, self.cloud_storage_address]  = self.get_addresses(required_addresses, 'Cell Reconstruction')\n",
    "#         self.em = 'brainmaps://10393113184:ell:roi450um_xyz'\n",
    "#         self.base_seg = 'brainmaps://10393113184:ell:roi450um_seg32fb16fb_220930'\n",
    "#         self.agglo_seg = 'brainmaps://10393113184:ell:roi450um_seg32fb16fb_220930:v230111c_16_strict_only_spl'\n",
    "\n",
    "        keys_base_segments = ['unknown','axon', 'basal dendrite', 'apical dendrite', 'dendrite', 'multiple']\n",
    "        keys_end_points = ['exit volume', 'natural end', 'uncertain', 'pre-synaptic', 'post-synaptic']\n",
    "        \n",
    "        agglo_seg_id = self.get_agglo_seg_of_base_seg(main_base_id)\n",
    "        \n",
    "        self.cell_data = {\n",
    "            'graph_edges': [],\n",
    "            'graph_nodes': [],\n",
    "            'base_locations': {},\n",
    "            'added_graph_edges': [], \n",
    "            'added_graph_edges_pre_proofreading': [],\n",
    "            'end_points': {key: [] for key in keys_end_points},\n",
    "            'base_seg_merge_points': [],\n",
    "            'removed_base_segs': set(),\n",
    "            'anchor_seg' : str(main_base_id),\n",
    "            'metadata': {   \n",
    "                'main_seg' : {'agglo' : {self.agglo_seg : agglo_seg_id}, 'base' : str(main_base_id)},\n",
    "                'data_sources': {\n",
    "                    'em' : self.em, \n",
    "                    'base': self.base_seg, \n",
    "                    'agglo': self.agglo_seg,\n",
    "                    },\n",
    "                'timing' : [],\n",
    "                'completion' : []\n",
    "                },\n",
    "            'base_segments' : {dtype: set() for dtype in keys_base_segments}\n",
    "        }\n",
    "        \n",
    "        self.get_vx_sizes('Cell Reconstruction')\n",
    "\n",
    "    def get_addresses(self, required_addresses, mode):\n",
    "        \n",
    "        '''\n",
    "        req_addresses = ['agglo_address', 'base_address', 'em_address', 'cloud_storage_address']\n",
    "        '''\n",
    "        a = ', '.join(required_addresses)\n",
    "\n",
    "        self.db_cursors[mode].execute(f'''SELECT {a} FROM addresses_table LIMIT 1''')\n",
    "\n",
    "        results = self.db_cursors[mode].fetchall()[0]\n",
    "\n",
    "        return results\n",
    "        \n",
    "    def update_selected_db(self, db_path, mode = 'Cell Reconstruction'):\n",
    "        '''\n",
    "        mode: 'Cell Reconstruction'\n",
    "        '''\n",
    "\n",
    "#         if not 'No file selected' in db_path: # cannot be 'No file selected' because doing it manually, not by gui\n",
    "\n",
    "        db_connection = sqlite3_connect(db_path, check_same_thread=False) \n",
    "        db_cursor = db_connection.cursor()\n",
    "\n",
    "        self.db_cursors[mode] = db_cursor\n",
    "        self.db_paths[mode] = db_path\n",
    "        self.settings_dict[mode]['cred'] = db_path\n",
    "        \n",
    "        db_path_to_display = db_path\n",
    "        \n",
    "        self.db_path_labels[mode]['text'] = db_path_to_display\n",
    "\n",
    "    def get_vx_sizes(self, mode):\n",
    "        \n",
    "        '''\n",
    "        mode : \"Network Exploration\" or \"Cell Reconstruction\"\n",
    "        '''\n",
    "        \n",
    "        self.db_cursors[mode].execute('SELECT * FROM voxel_sizes_table')\n",
    "\n",
    "        self.vx_sizes = {}\n",
    "\n",
    "        for dtype, x, y, z, x_size, y_size, z_size in self.db_cursors[mode].fetchall():\n",
    "\n",
    "            self.vx_sizes[dtype] = [x, y, z]\n",
    "\n",
    "            if dtype == 'em':\n",
    "                self.starting_location = [int(x_size/2), int(y_size/2), int(z_size/2),]\n",
    "\n",
    "        \n",
    "    def import_base_segments(self,base_segments):\n",
    "        \n",
    "        '''\n",
    "        base_segments is the list of segments from neuroglancer json (which is why get put in \"unknown\")\n",
    "        '''\n",
    "        # Turn lists back to sets:\n",
    "        self.cell_data['base_segments']['unknown'] = set([str(x) for x in base_segments])\n",
    "                \n",
    "    \n",
    "    def create_pr_graph(self):\n",
    "\n",
    "        seg_id = self.cell_data['metadata']['main_seg']['base']\n",
    "\n",
    "        print(f'Creating base segment graph for cell {seg_id}', 'Cell Reconstruction')\n",
    "\n",
    "        all_base_segs = [str(a) for b in self.cell_data['base_segments'].values() for a in b]\n",
    "        \n",
    "        self.update_base_locations(all_base_segs)\n",
    "\n",
    "              \n",
    "        ####\n",
    "        # Correct base segment locations that got left out \n",
    "        '''td:\n",
    "        figure out why they are left out.\n",
    "        for example, for one the segment id returned '0' even though there was a location returned\n",
    "        '''\n",
    "        no_loc_base_segs = [str(x) for x in all_base_segs if x not in self.cell_data['base_locations']]\n",
    "        for s in no_loc_base_segs:\n",
    "            results_dict = self.get_locations_from_base_segs(s)\n",
    "            k = list(results_dict.keys())[0] # get key for this segment ID in queried segment location\n",
    "            self.cell_data['base_locations'][s] = self.get_corrected_xyz(results_dict[k], 'seg') # manually log its location with given segment ID\n",
    "        ####\n",
    "        \n",
    "        \n",
    "        possible_edges = []\n",
    "        agglo_segs_done = set()\n",
    "        base_segs_done = set()\n",
    "\n",
    "        for base_seg in all_base_segs:\n",
    "\n",
    "            if base_seg in base_segs_done: continue\n",
    "\n",
    "            agglo_seg = self.get_agglo_seg_of_base_seg(base_seg)\n",
    "            children_base_segs = self.get_base_segs_of_agglo_seg(agglo_seg)\n",
    "            base_segs_done.update(children_base_segs)\n",
    "\n",
    "            if not agglo_seg in agglo_segs_done:\n",
    "\n",
    "                edges = self.get_edges_from_agglo_seg(agglo_seg)\n",
    "\n",
    "                agglo_segs_done.add(agglo_seg)\n",
    "                possible_edges.extend(edges)\n",
    "\n",
    "        all_bs_set = set(all_base_segs)\n",
    "        possible_edges = [x for x in possible_edges if x[0] in all_bs_set]\n",
    "        chosen_edges = [x for x in possible_edges if x[1] in all_bs_set]\n",
    "\n",
    "        self.pr_graph = ig_Graph(directed=False)\n",
    "        self.pr_graph.add_vertices(all_base_segs)\n",
    "        self.pr_graph.add_edges(chosen_edges)\n",
    "\n",
    "        self.add_cc_bridging_edges_pairwise()\n",
    "        self.attach_noloc_segs()\n",
    "\n",
    "        '''\n",
    "        # removed assertion of pr_graph.clusters==1 because importing from a neuroglancer json might \"break\" this and it is ok...\n",
    "        \n",
    "        assert len(self.pr_graph.clusters(mode='weak')) == 1\n",
    "        '''\n",
    "        \n",
    "        n_clusters = len(self.pr_graph.clusters(mode='weak'))\n",
    "        \n",
    "        print(f'{n_clusters} clusters in graph (note should/would be only 1 if loaded base ID from agglomo fresh)')\n",
    "        \n",
    "        self.assert_segs_in_sync()\n",
    "        \n",
    "        print(f'successful assertion that graph segments and segments listed in base_segments match')\n",
    "\n",
    "    def save_cell_graph(self, directory_path = None, file_name=None, save_to_cloud=False):\n",
    "        \n",
    "        cell_data = deepcopy(self.cell_data)\n",
    "        cell_data['graph_nodes'] = [x['name'] for x in self.pr_graph.vs]\n",
    "        cell_data['graph_edges'] = [(self.pr_graph.vs[x.source]['name'], self.pr_graph.vs[x.target]['name']) for x in self.pr_graph.es]\n",
    "\n",
    "        # Convert sets to lists for saving in json file:\n",
    "        for dtype in cell_data['base_segments'].keys():\n",
    "            cell_data['base_segments'][dtype] = list(cell_data['base_segments'][dtype])\n",
    "        \n",
    "        cell_data['removed_base_segs'] = list(cell_data['removed_base_segs'])\n",
    "\n",
    "        timestamp = str(datetime.now())[:-7].replace(':','.')\n",
    "        main_base_id = cell_data['metadata']['main_seg']['base']\n",
    "\n",
    "        completion_list = list(set(cell_data['metadata']['completion']))\n",
    "        completion_list.sort()\n",
    "        completion_string = ','.join(completion_list).replace('_', ' ')\n",
    "\n",
    "        if directory_path == None:\n",
    "            directory_path = Path().absolute()\n",
    "            \n",
    "        if file_name == None:\n",
    "            file_name = f'cell_graph_{main_base_id}_{completion_string}_{timestamp}.json'\n",
    "\n",
    "        cell_data['metadata']['data_sources']['agglo'] = self.agglo_seg\n",
    "\n",
    "#         with open(f'{self.save_dir}/{file_name}', 'w') as fp:\n",
    "#             json_dump(cell_data, fp)\n",
    "        with open(directory_path / file_name, 'w') as fp:\n",
    "            json.dump(cell_data, fp)\n",
    "\n",
    "        print(f'Saved cell {main_base_id} locally', 'Cell Reconstruction')\n",
    "\n",
    "    def update_base_locations(self, seg_list):\n",
    "\n",
    "        seg_list = [x for x in seg_list if x not in self.cell_data['base_locations'].keys()]\n",
    "\n",
    "        result_dict = self.get_locations_from_base_segs(seg_list)\n",
    "\n",
    "        for r in result_dict:\n",
    "            self.cell_data['base_locations'][r] = self.get_corrected_xyz(result_dict[r], 'seg')\n",
    "\n",
    "    def get_locations_from_base_segs(self, base_segs, batch_size = 1000):\n",
    "\n",
    "        results = {}\n",
    "\n",
    "        if len(base_segs) > 0:\n",
    "        \n",
    "            num_batches = int(len(base_segs)/batch_size)\n",
    "            \n",
    "            for batch in range(num_batches+1):\n",
    "\n",
    "                q = ','.join([str(x) for x in base_segs[batch*batch_size:(batch+1)*batch_size]])\n",
    "                \n",
    "                query = f\"\"\"SELECT seg_id, x, y, z FROM base_location WHERE seg_id IN ({q})\"\"\"\n",
    "\n",
    "                self.db_cursors['Cell Reconstruction'].execute(query)\n",
    "\n",
    "                this_batch = {str(x[0]): (int(x[1]), int(x[2]), int(x[3])) for x in self.db_cursors['Cell Reconstruction'].fetchall()}\n",
    "\n",
    "                results.update(this_batch)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def get_corrected_xyz(self, xyz, adj_key, rel_to_em=False):\n",
    "\n",
    "        result = []\n",
    "\n",
    "        for pos, coord in enumerate(xyz):\n",
    "            result.append(coord*self.vx_sizes[adj_key][pos])\n",
    "            \n",
    "        if rel_to_em==True:\n",
    "            result = [int(result[x]/self.vx_sizes['em'][x]) for x in range(3)]\n",
    "\n",
    "        return result\n",
    "\n",
    "    def get_agglo_seg_of_base_seg(self, base_seg):\n",
    "\n",
    "        base_seg = str(base_seg)\n",
    "\n",
    "        query = f\"\"\"SELECT agglo_id FROM agglo_base_resolved WHERE base_id = {base_seg}\"\"\"\n",
    "\n",
    "        self.db_cursors['Cell Reconstruction'].execute(query)\n",
    "        agglo_segs = [str(x[0]) for x in self.db_cursors['Cell Reconstruction'].fetchall()]\n",
    "\n",
    "        assert len(agglo_segs) <= 1\n",
    "\n",
    "        if agglo_segs == []:\n",
    "            return base_seg\n",
    "        else:\n",
    "            return agglo_segs[0]\n",
    "\n",
    "    def get_base_segs_of_agglo_seg(self, agglo_seg):\n",
    "\n",
    "        agglo_seg = str(agglo_seg)\n",
    "\n",
    "        query = f\"\"\"SELECT base_id FROM agglo_base_resolved WHERE agglo_id = {agglo_seg}\"\"\"\n",
    "\n",
    "        self.db_cursors['Cell Reconstruction'].execute(query)\n",
    "        base_segs = [str(x[0]) for x in self.db_cursors['Cell Reconstruction'].fetchall()]\n",
    "        base_segs.append(agglo_seg)\n",
    "\n",
    "        return base_segs\n",
    "\n",
    "    def get_edges_from_agglo_seg(self, agglo_seg):\n",
    "\n",
    "        agglo_seg = str(agglo_seg)\n",
    "\n",
    "        query = f\"\"\"SELECT label_a, label_b FROM agglo_to_edges WHERE agglo_id = {agglo_seg}\"\"\"\n",
    "\n",
    "        self.db_cursors['Cell Reconstruction'].execute(query)\n",
    "        edges = [(str(x[0]), str(x[1])) for x in self.db_cursors['Cell Reconstruction'].fetchall()]\n",
    "\n",
    "        return edges\n",
    "    \n",
    "    def add_cc_bridging_edges_pairwise(self):\n",
    "        \n",
    "        '''\n",
    "        con_comms = \"connected components\" abbreviation\n",
    "        '''\n",
    "\n",
    "        con_comms = list(self.pr_graph.clusters(mode='weak'))\n",
    "\n",
    "        while len(con_comms) > 1:\n",
    "\n",
    "            candidate_edges = []\n",
    "\n",
    "            for cc1, cc2 in combinations(con_comms, 2): # gets all possible pairwise combinations between segments\n",
    "                \n",
    "                # get the name of each base segment\n",
    "                cc1_base_segs = [self.pr_graph.vs[x]['name'] for x in cc1]\n",
    "                cc2_base_segs = [self.pr_graph.vs[x]['name'] for x in cc2]\n",
    "\n",
    "                cc1_list = [x for x in cc1_base_segs if x in self.cell_data['base_locations']]\n",
    "                cc2_list = [x for x in cc2_base_segs if x in self.cell_data['base_locations']]\n",
    "\n",
    "                if cc1_list == [] or cc2_list == []:\n",
    "                    continue\n",
    "\n",
    "                sel_cc1, sel_cc2, dist = self.get_closest_dist_between_ccs(cc1_list, cc2_list)\n",
    "                candidate_edges.append([sel_cc1, sel_cc2, dist])\n",
    "\n",
    "            if candidate_edges == []: \n",
    "                return\n",
    "\n",
    "            origin, target, dist = min(candidate_edges, key = lambda x: x[2])\n",
    "\n",
    "            self.pr_graph.add_edges([(origin, target)])\n",
    "            self.cell_data['added_graph_edges_pre_proofreading'].append([origin, target, dist])\n",
    "#             self.update_mtab(f'Added an edge between segments {origin} and {target}, {dist} nm apart', 'Cell Reconstruction')\n",
    "\n",
    "            con_comms = list(self.pr_graph.clusters(mode='weak'))\n",
    "\n",
    "    def get_closest_dist_between_ccs(self, cc1_node_list, cc2_node_list):\n",
    "\n",
    "        cc1_node_locs = [self.cell_data['base_locations'][x] for x in cc1_node_list]\n",
    "        cc2_node_locs = [self.cell_data['base_locations'][x] for x in cc2_node_list]\n",
    "\n",
    "        f = cdist(cc1_node_locs, cc2_node_locs, 'euclidean')\n",
    "\n",
    "        min_indices = unravel_index(argmin(f, axis=None), f.shape)\n",
    "\n",
    "        sel_cc1 = cc1_node_list[min_indices[0]]\n",
    "        sel_cc2 = cc2_node_list[min_indices[1]]\n",
    "        dist = int(f[min_indices])  \n",
    "\n",
    "        return sel_cc1, sel_cc2, dist\n",
    "            \n",
    "    def attach_noloc_segs(self):\n",
    "        ''' NOTE that this does not run (it returns) if self.pr_graph.clusters(mode='weak') == 1\n",
    "        This is a case that is asserted in oringinal CREST.py in '''\n",
    "        \n",
    "        # For isolated segments without locations, attach to largest connected component:\n",
    "        remaining_cc = list(self.pr_graph.clusters(mode='weak'))\n",
    "\n",
    "        if len(remaining_cc) == 1: return\n",
    "\n",
    "        if len(remaining_cc) > 1:\n",
    "            no_loc_base_segs = set([x['name'] for x in self.pr_graph.vs if x['name'] not in self.cell_data['base_locations']])\n",
    "            largest_cc = max(remaining_cc, key = lambda x: len(x))\n",
    "            for cc in remaining_cc:\n",
    "                no_loc_this_cc = cc & no_loc_base_segs\n",
    "                if cc != largest_cc and no_loc_this_cc != set():\n",
    "                    rand_seg1 = random_choice(list(no_loc_this_cc))\n",
    "                    rand_seg2 = random_choice(list(largest_cc))\n",
    "                    self.pr_graph.add_edges([(rand_seg1, rand_seg2)])\n",
    "                    self.cell_data['added_graph_edges_pre_proofreading'].append([rand_seg1, rand_seg2, 'unknown'])\n",
    "#                     print(f'Added an edge between segments {rand_seg1} and {rand_seg2}', 'Cell Reconstruction')\n",
    "\n",
    "    def assert_segs_in_sync(self, return_segs=False):\n",
    "\n",
    "#         displayed_segs = set([str(x) for x in self.viewer.state.layers['base_segs'].segments]) # not connected to neuroglancer, so not relevant\n",
    "        graph_segs = set([x['name'] for x in self.pr_graph.vs])\n",
    "        listed_segs = set([a for b in [self.cell_data['base_segments'][cs] for cs in self.cell_data['base_segments'].keys()] for a in b])\n",
    "\n",
    "        assert listed_segs == graph_segs\n",
    "\n",
    "#         if not displayed_segs == graph_segs:\n",
    "#             self.update_displayed_segs()\n",
    "        \n",
    "        if return_segs:\n",
    "#             return displayed_segs\n",
    "            return graph_segs\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "    def import_annotations(self,neuroglancer_data, neuroglancer_layer_name, crest_layer_name):\n",
    "        \n",
    "        for n, c in zip(neuroglancer_layer_name,crest_layer_name):\n",
    "            \n",
    "            # get the 'layers' dictionary that has that name\n",
    "\n",
    "            neuroglancer_layer = next((item for item in neuroglancer_data['layers'] if item[\"name\"] == n), None)\n",
    "\n",
    "            # create the annotation list for CREST and put it into cell_data\n",
    "\n",
    "            annotation_list = []\n",
    "\n",
    "            for v in neuroglancer_layer['annotations']:\n",
    "                # print(v)\n",
    "                corrected_location = self.get_corrected_xyz(v['point'], 'seg')\n",
    "\n",
    "                if 'segments' not in v.keys():\n",
    "                    annotation_list.extend([corrected_location])\n",
    "                if 'segments' in v.keys():\n",
    "                    annotation_list.extend([corrected_location + v['segments'][0]])\n",
    "\n",
    "            self.cell_data['end_points'][c]=annotation_list\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc18eab-9366-478a-bfbc-0e3bb4eef890",
   "metadata": {},
   "source": [
    "### Path definitions and filename extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8b231dc-6094-449a-a0d1-65fd47c7b0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = \"D:\\\\electric-fish\\\\eCREST_local-files\\\\neuroglancer-json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "758d3dfc-1ae4-4497-8f00-ccba806a81d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cell_216129202_type_MG2_v2_NS.json',\n",
       " 'cell_300316308_type_MG2_v2_NS.json',\n",
       " 'cell_300380579_type_MG1_NS.json',\n",
       " 'cell_387368998_type_MG1_v8_NS_MP.json',\n",
       " 'cell_42802314_type_MG2_NS.json',\n",
       " 'cell_472175645_type_MG1_mpg.json',\n",
       " 'cell_472409584_type_MG1_v2_NS.json']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p = Path(dirname)\n",
    "names = [p_.name for p_ in list(p.glob('cell_*'))] # get all the filenames in the directory that start with \"cell_\"\n",
    "\n",
    "display(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4270ef24-be3a-4f61-a267-529b84df2cf9",
   "metadata": {},
   "source": [
    "### Set path to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "aaa4e6f7-e501-4423-bb45-846b333139b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = Path(\"D:\\\\electric-fish\\\\eCREST_local-files\\\\Mariela_bigquery_exports_agglo_v230111c_16_crest_proofreading_database.db\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add9f618-7956-464f-b4b8-dd374d92a754",
   "metadata": {},
   "source": [
    "## Get info from the base neuroglancer json you want converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "fd4c1f28-c90c-4c2f-8888-6b7c36ff6c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you have selected cell 472409584 to convert\n"
     ]
    }
   ],
   "source": [
    "# Find all segments for a specific cell ID\n",
    "\n",
    "# key = '305332461'\n",
    "# f = d[key][1]\n",
    "\n",
    "# f = names[2] # for now, index based on which filename you want from the list\n",
    "f = 'cell_472409584_type_MG1_v2_NS.json'\n",
    "\n",
    "with open(p / f, 'r') as myfile: # 'p' is the dirpath and 'f' is the filename from the created 'd' dictionary\n",
    "    data=myfile.read()\n",
    "neuroglancer_data = json.loads(data)\n",
    "\n",
    "base_segment_ID = f.split('_')[1] # gets the base segment ID from the name\n",
    "\n",
    "print(f'you have selected cell {base_segment_ID} to convert')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bce5d5-11fb-482b-9a68-4ce4b358ea76",
   "metadata": {},
   "source": [
    "### Obtain the list of base_segments from the neuroglancer json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "3372a150-e65b-4117-9f02-5ab88a6f9fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_segment_list_ng = neuroglancer_data['layers'][1]['segments']\n",
    "main_base_id = base_segment_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92702055-22d2-4eb0-95f0-7ab16d15e06f",
   "metadata": {},
   "source": [
    "## Use agglomeration database to get info needed for CREST.json format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17ee431-675e-4a80-aa0e-f449ae8be2a2",
   "metadata": {},
   "source": [
    "## Crest an instance of the crest_json class\n",
    "\n",
    "Initialize with the main_base_id from the neuroglancer file you are converting.\n",
    "\n",
    "Use the db_path to the agglomeration dataset (must be local on computer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "0630e21f-2aea-40f7-9ea5-2b27ada5e05a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating CREST file for 472409584 Cell Reconstruction\n"
     ]
    }
   ],
   "source": [
    "crest = crest_json(main_base_id, db_path)\n",
    "\n",
    "## Note cloud storage address ;)\n",
    "# crest.cloud_storage_address   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e362668-9e5e-47e6-9bde-0630e98864bb",
   "metadata": {},
   "source": [
    "## Initialize base segments and graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e2863a-6378-40fa-8db8-a37c13460b8f",
   "metadata": {},
   "source": [
    "### Import all the neurglancer-defined base segments into crest_json object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "97c8c4a7-2975-4134-a4bf-be36b1faa36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "crest.import_base_segments(base_segment_list_ng)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6d1854-00b1-4030-ae2b-76b75708dbe1",
   "metadata": {},
   "source": [
    "### Create the graph from the base segments\n",
    "\n",
    "Note that this step also populates 'base_locations'.\n",
    "\n",
    "> Note this script was modified to address base segment locations not associated  correctly with their base_seg_id in sql database?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "c05038d7-a13e-4b8a-ab4e-afd038a9bf34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating base segment graph for cell 472409584 Cell Reconstruction\n",
      "1 clusters in graph (note should/would be only 1 if loaded base ID from agglomo fresh)\n",
      "successful assertion that graph segments and segments listed in base_segments match\n"
     ]
    }
   ],
   "source": [
    "crest.create_pr_graph()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8751f56-5cf3-4d81-bf97-5810274d874a",
   "metadata": {},
   "source": [
    "## Get annotations from neuroglancer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99fc65c-2f82-42c2-8376-1a0933dbe329",
   "metadata": {},
   "source": [
    "### First, specify the annotation layer names in neuroglancer and crest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "29dbae77-9a43-4aa5-8042-078eba8e7729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the original neuroglancer layer name\n",
    "neuroglancer_layer_name = ['synapses','annotations']\n",
    "\n",
    "# Define the 'end_points' annotation layer names to populate for CREST\n",
    "crest_layer_name = ['post-synaptic','natural end']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e30cd8-619d-4019-a135-8a544c755b06",
   "metadata": {},
   "source": [
    "### Then, transfer from neuroglancer to crest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "76548116-d35b-4ab6-a320-17eb3dcadfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "crest.import_annotations(neuroglancer_data, neuroglancer_layer_name, crest_layer_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3da8ee-f0bf-4d09-88fb-6f9fbde33d5a",
   "metadata": {},
   "source": [
    "## Save new json\n",
    "\n",
    "This step populates the cell_data graph data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "17f3d053-d4b8-494a-be87-2eda5b667097",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = Path(\"D:\\\\electric-fish\\\\eCREST_local-files\\\\neuroglancer-json\\\\to-crest-json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "d500cab1-ac11-4509-9b87-ec72b46fda47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cell 472409584 locally Cell Reconstruction\n"
     ]
    }
   ],
   "source": [
    "crest.save_cell_graph(directory_path = directory_path) # If do not give file_path, then it will auto-generate one like CREST produces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21516cc1-aa10-4222-b4a1-c96b30ad4777",
   "metadata": {},
   "source": [
    "## Other..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279b6428-52f0-45fc-9b1b-6af275b2e335",
   "metadata": {},
   "source": [
    "Crest does something similar when reading from list of dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21245eb4-d36d-416a-a2fa-adc0a829179c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if type(self.cells_todo) == dict:\n",
    "    self.cells_todo = {str(seg_id): {cell_struc: set([str(a) for a in self.cells_todo[seg_id][cell_struc]]) for cell_struc in self.cells_todo[seg_id]} for seg_id in self.cells_todo.keys()}\n",
    "    '''\n",
    "    ###... from above (to read easier)\n",
    "    for seg_id in self.cells_todo.keys():\n",
    "        for cell_struc in self.cells_todo[seg_id]}:\n",
    "            for a in self.cells_todo[seg_id][cell_struc]]):\n",
    "                self.cells_todo = {str(seg_id): {cell_struc: set([str(a) # creates a set() of all segments in the cell structure\n",
    "    ###\n",
    "    \n",
    "    so, self.cells_todo is a dictionary of {'main_base_segment_ID'}{'unknown' : set(base_segments), ...'other cell structures' : set(base_segments)}\n",
    "    '''\n",
    "\n",
    "# Get all file names from cloud for seg_ids of interest:\n",
    "all_cloud_file_names = [x.name for x in self.proofread_files_bucket.list_blobs() if x.name.split('_')[2] in self.cells_todo]\n",
    "all_local_file_names = [x for x in listdir(self.save_dir) if 'cell' in x and x.split('_')[2] in self.cells_todo]\n",
    "\n",
    "#self.completion_message_dict = {}\n",
    "\n",
    "all_cloud_seg_ids = set([x.split('_')[2] for x in all_cloud_file_names])\n",
    "all_local_seg_ids = set([x.split('_')[2] for x in all_local_file_names])\n",
    "cells_with_files = all_cloud_seg_ids.union(all_local_seg_ids)\n",
    "cells_without_files = [x for x in self.cells_todo if x not in cells_with_files]\n",
    "\n",
    "num_fileless_cells = len(cells_without_files)\n",
    "\n",
    "if num_fileless_cells > 0:\n",
    "    self.update_mtab(f'No starting file found locally or in cloud for {num_fileless_cells} cells', 'Cell Reconstruction')\n",
    "else:\n",
    "    self.update_mtab(f'Starting files found for all cells, checking completion status of each file ...', 'Cell Reconstruction')\n",
    "\n",
    "complete_cells = []\n",
    "\n",
    "for seg_id in self.cells_todo: # if cells_todo is a dict, then seg_id will be the dict key\n",
    "\n",
    "    '''\n",
    "    This is just for if the cells already have a file... if import from neuroglancer dict, they will not\n",
    "    \n",
    "    # If a seg ID already has a file, we need to choose which one to use\n",
    "    if seg_id in cells_with_files:\n",
    "\n",
    "        if self.most_recent_file_complete(seg_id, ['local']) and specific_file == None:\n",
    "            complete_cells.append(seg_id)\n",
    "            #msg = f'Cell {seg_id} has already been completed for the selected cell structures locally'\n",
    "            #self.update_mtab(msg, 'Cell Reconstruction')\n",
    "            #self.completion_message_dict[seg_id] = msg\n",
    "            continue\n",
    "\n",
    "        # If no complete cell locally, start most recent file, whether it originates from cloud or local:\n",
    "\n",
    "        if specific_file != None:\n",
    "            most_recent_file = file_name\n",
    "        else:\n",
    "            most_recent_file = self.get_most_recent_cell_files(seg_id, ['cloud', 'local'])[0]\n",
    "\n",
    "        if most_recent_file in listdir(self.save_dir):\n",
    "\n",
    "            msg = f'Cell {seg_id} not completed for all the selected cell structures in the most recent (local) version'\n",
    "\n",
    "            file_source = 'local'\n",
    "\n",
    "        else:\n",
    "\n",
    "            this_seg_cloud_file_names = [x.name for x in self.proofread_files_bucket.list_blobs() if x.name.split('_')[2] == str(seg_id)]\n",
    "\n",
    "            assert most_recent_file in this_seg_cloud_file_names\n",
    "\n",
    "            file_source = 'cloud'\n",
    "\n",
    "            if self.most_recent_file_complete(seg_id, ['cloud']):\n",
    "                msg = f'Cell {seg_id} completed for all the selected cell structures in the most recent (cloud) version'\n",
    "            else:\n",
    "                msg = f'Cell {seg_id} not completed for all the selected cell structures in the most recent (cloud) version'\n",
    "\n",
    "        #self.update_mtab(msg, 'Cell Reconstruction')\n",
    "        #self.completion_message_dict[seg_id] = msg\n",
    "\n",
    "        if file_source == 'cloud':\n",
    "\n",
    "            try:\n",
    "                blob = self.proofread_files_bucket.blob(most_recent_file)\n",
    "                blob.download_to_filename(f'{self.save_dir}/{most_recent_file}')\n",
    "\n",
    "            except ConnectionError:\n",
    "                self.create_cloud_storage_client()\n",
    "                blob = self.proofread_files_bucket.blob(most_recent_file)\n",
    "                blob.download_to_filename(f'{self.save_dir}/{most_recent_file}')\n",
    "\n",
    "            self.update_mtab(f'Proofread cell file {most_recent_file} downloaded from cloud', 'Cell Reconstruction')\n",
    "\n",
    "        with open(f'{self.save_dir}/{most_recent_file}', 'r') as fp:\n",
    "            self.cell_data = json_load(fp)\n",
    "\n",
    "        # If agglo_id has changed from last time, add new base segments - currently disabled:\n",
    "        last_agglo_id = self.cell_data['metadata']['data_sources']['agglo']\n",
    "        changed_agglo_id = (last_agglo_id != self.agglo_seg)\n",
    "\n",
    "        changed_agglo_id = False\n",
    "\n",
    "        if changed_agglo_id:\n",
    "\n",
    "            self.add_new_base_segs_from_new_agglo(seg_id)\n",
    "\n",
    "            # Wipe clean the stored graph:\n",
    "            self.cell_data['graph_edges'] = []\n",
    "            self.cell_data['graph_nodes'] = []\n",
    "\n",
    "            self.create_pr_graph()\n",
    "            self.save_cell_graph()\n",
    "    '''\n",
    "\n",
    "    # Otherwise, it depends on whether the input cells todo is a list or dictionary:\n",
    "    else:\n",
    "        self.making_starting_cell_data(seg_id)\n",
    "        '''basically, \n",
    "            - sets up self.cell_data with default keys\n",
    "            - imports addresses\n",
    "            - takes 'unknown' key:value pair dictionary in self.cells_todo dictionary and puts that into self.cell_data['base_segments']\n",
    "                 -- self.cell_data['base_segments'] = self.cells_todo[main_base_id]\n",
    "        '''\n",
    "\n",
    "        \n",
    "        ''' \n",
    "        # self.pre_load_edges seems set to 0 and not changed\n",
    "        # and self.get_new_gen_dict_entries function definition seems commented out\n",
    "        \n",
    "        if self.pre_load_edges == 1:\n",
    "            all_base_segs = [a for b in self.cell_data['base_segments'].values() for a in b]\n",
    "            self.get_new_gen_dict_entries(all_base_segs, 0)\n",
    "        '''\n",
    "        \n",
    "        self.create_pr_graph()\n",
    "        '''\n",
    "        - gets all base segments from ['base_segments'][all keys]\n",
    "        - self.update_base_locations(all_base_segs)\n",
    "            -- populates cell_data['base_locations']\n",
    "            -- self.get_locations_from_base_segs(seg_list not already in 'base_locations')\n",
    "            -- self.cell_data['base_locations'][r] = self.get_corrected_xyz(result_dict[r], 'seg') \n",
    "                --- adjusts xyz based on resolution per voxel? coord*self.vx_sizes['seg'][x,y, or z]\n",
    "\n",
    "        possible_edges = []\n",
    "        agglo_segs_done = set()\n",
    "        base_segs_done = set()\n",
    "        \n",
    "        for base_seg in all_base_segs: # getting all of the agglomeration segments with base segments in them and getting their edges\n",
    "                                        # (keeping a list of base_seg_done to not double-do it because many base segments per agglo seg)\n",
    "\n",
    "            if base_seg in base_segs_done: continue\n",
    "\n",
    "            agglo_seg = self.get_agglo_seg_of_base_seg(base_seg) \n",
    "            children_base_segs = self.get_base_segs_of_agglo_seg(agglo_seg)\n",
    "            base_segs_done.update(children_base_segs)\n",
    "\n",
    "            if not agglo_seg in agglo_segs_done:\n",
    "\n",
    "                edges = self.get_edges_from_agglo_seg(agglo_seg) # so edges are among base_segments within an agglomeration segment?\n",
    "                                                                # query = f\"\"\"SELECT label_a, label_b FROM agglo_to_edges WHERE agglo_id = {agglo_seg}\"\"\"\n",
    "                \n",
    "                agglo_segs_done.add(agglo_seg)\n",
    "                possible_edges.extend(edges)\n",
    "\n",
    "        all_bs_set = set(all_base_segs)\n",
    "        possible_edges = [x for x in possible_edges if x[0] in all_bs_set]  # first, make sure the first vertex is in all_base_segments\n",
    "        chosen_edges = [x for x in possible_edges if x[1] in all_bs_set]    # second, make sure the second vertex is in all_base_segments\n",
    "\n",
    "        self.pr_graph = ig_Graph(directed=False) # create a graph object\n",
    "        self.pr_graph.add_vertices(all_base_segs) # add base segments as vertices\n",
    "        self.pr_graph.add_edges(chosen_edges)  # add edges\n",
    "\n",
    "        self.add_cc_bridging_edges_pairwise()\n",
    "            -- if there is more than one connected component (so if more than one agglomo in reconstruction maybe?) then it connects them between the closes base segments among them\n",
    "            -- calculates distance between base segments in each cluster from list(self.pr_graph.clusters(mode='weak'))\\\n",
    "            -- repeats until only one cluster in graph\n",
    "            \n",
    "        self.attach_noloc_segs()\n",
    "\n",
    "        assert len(self.pr_graph.clusters(mode='weak')) == 1\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        self.save_cell_graph()\n",
    "        '''\n",
    "        populates 'graph_nodes' and 'graph_edges'\n",
    "        '''\n",
    "\n",
    "\n",
    "    self.cells_todo_d[seg_id] = self.cell_data['base_segments']\n",
    "\n",
    "# Save new settings file for quick completion lookup next time:\n",
    "with open(f'{self.script_directory}/CREST_settings.json', 'w') as fp:\n",
    "    json_dump(self.settings_dict, fp)\n",
    "\n",
    "# Make sure cells todo is a list:\n",
    "self.cells_todo = [x for x in self.cells_todo if x not in complete_cells]\n",
    "\n",
    "if specific_file == None:\n",
    "    self.remove_skipped_cells()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
