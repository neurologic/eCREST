{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab640bc1-30fd-4769-8699-0531bc6412a0",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "382c671b-8ae7-4475-b52d-a245a12177fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################################################ \n",
    "# Get the latest CREST files for each ID within the target folder (dirname)\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "from sqlite3 import connect as sqlite3_connect\n",
    "from sqlite3 import DatabaseError\n",
    "from igraph import Graph as ig_Graph\n",
    "from igraph import plot as ig_plot\n",
    "from scipy.spatial.distance import cdist\n",
    "from random import choice as random_choice\n",
    "from itertools import combinations\n",
    "from numpy import array, unravel_index, argmin, mean,unique,nan\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "import neuroglancer\n",
    "from webbrowser import open as wb_open\n",
    "from webbrowser import open_new as wb_open_new\n",
    "import neuroglancer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebab6d25-fb65-4d34-9795-c93e88790bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/kperks/Documents/ell-connectome/eCREST/eCREST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c9fe444-56db-4da9-b56f-898d132af90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from eCREST_cli_beta import ecrest, import_settings\n",
    "from eCREST_cli import ecrest, import_settings, get_cell_filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13667499-d2d9-4978-a334-578a5896c49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_segments_dict(dirpath):\n",
    "\n",
    "    nodefiles = [child.name for child in sorted(dirpath.iterdir()) if (child.name[0]!='.') & (child.is_file()) & (\"desktop\" not in child.name)]\n",
    "\n",
    "    # Create a base_segments dictionary of all cells in the directory\n",
    "    base_segments = {}\n",
    "    for x in nodefiles:\n",
    "        # print(x)\n",
    "        with open(dirpath / x, 'r') as myfile: # 'p' is the dirpath and 'f' is the filename from the created 'd' dictionary\n",
    "            cell_data=myfile.read()\n",
    "            cell_data = json.loads(cell_data)\n",
    "        base_segments[x] = set([a for b in cell_data['base_segments'].values() for a in b]) #cell.cell_data['base_segments']\n",
    "        # base_segments[x] = set([a for b in cell_data['base_segments'].values() for a in b]) #cell.cell_data['base_segments']\n",
    "\n",
    "    return base_segments\n",
    "\n",
    "def check_duplicates(base_segments):\n",
    "    '''\n",
    "    base_segments is a dictionary of all segments that this script checks among\n",
    "    '''\n",
    "    df_all = pd.DataFrame()\n",
    "    for _,this_cell in base_segments.items():\n",
    "        overlap = []\n",
    "        num_dup = []\n",
    "        for x in base_segments.keys():\n",
    "            overlap.append(len(this_cell&base_segments[x])/len(base_segments[x]))\n",
    "            num_dup.append(len(this_cell&base_segments[x]))\n",
    "\n",
    "        df = pd.DataFrame({\n",
    "            \"self\": _,\n",
    "            \"dups\": list(base_segments.keys()),\n",
    "            \"overlap-percent\": overlap,\n",
    "            \"number_seg_lap\": num_dup\n",
    "            }).replace(0, nan, inplace=False).dropna()\n",
    "        df = df[df['dups'] != _]\n",
    "        if not df.empty:\n",
    "            df_all = pd.concat([df_all,df]) \n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edbd11d-dfcb-4187-8721-4b8442429d33",
   "metadata": {},
   "source": [
    "The 'ecrest' class has been imported from eCREST_cli.py\n",
    "\n",
    "An instance of this object will be able to do things like:\n",
    "- open an neuroglancer viewer for proofrieading (see \"Proofread using CREST\")\n",
    "    - add-remove segments (using graph feature for efficiency)\n",
    "    - format itself and save itself as a CREST-style .json\n",
    "- add or remove annotation layers (see \"Annotation Layers\")\n",
    "- check for overlap with other .json files in a directory folder (see \"check duplicates\")\n",
    "- label cell structures\n",
    "- add base_segments from a list (see \"add segments\")\n",
    "- import annotations from another file (see \"Annotation Import\")\n",
    "- convert from neuroglancer json (see \"Convert From Neuroglancer to eCREST\")\n",
    "    - format itself and save itself as a CREST-style .json\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783b3095-f8f0-4fd7-af28-432ebf417b45",
   "metadata": {},
   "source": [
    "# Settings definitions\n",
    "\n",
    "Whether you are converting from neuroglancer or creating a new reconstruction, the settings_dict parameters is needed to create CREST json files with correct formatting. \n",
    "- 'save_dir' : the default directory where eCREST reconstructions are saved as JSON files\n",
    "- 'db_path' : the path to the agglomeration database file on the local computer (a copy is in the Google Drive). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8dca3add-a661-445b-b43c-f1992033e7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_settings_json = '<path to google drive on the local computer>/My Drive/ELL_connectome/Ela/settings_dict_Ela.json'\n",
    "path_to_settings_json = '/Users/kperks/Documents/ell-connectome/eCREST-local-files/settings_dict.json'\n",
    "settings_dict = import_settings(path_to_settings_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e13f4391-6bda-40c2-b700-a435d25f3ebb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'save_dir': '/Users/kperks/Library/CloudStorage/GoogleDrive-sawtelllab@gmail.com/My Drive/ELL_connectome/CREST_reconstructions/mg-network',\n",
       " 'max_num_base_added': 1000,\n",
       " 'cell_structures': ['unknown',\n",
       "  'axon',\n",
       "  'basal dendrite',\n",
       "  'apical dendrite',\n",
       "  'dendrite',\n",
       "  'multiple'],\n",
       " 'annotation_points': ['exit volume',\n",
       "  'natural end',\n",
       "  'uncertain',\n",
       "  'pre-synaptic',\n",
       "  'post-synaptic'],\n",
       " 'db_path': '/Users/kperks/Documents/ell-connectome/eCREST-local-files/Mariela_bigquery_exports_agglo_v230111c_16_crest_proofreading_database.db'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "settings_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243a5e4a-f495-427b-9645-06bc01772c4c",
   "metadata": {},
   "source": [
    "# Get base_segments dictionaries "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6028abfd-8581-41f7-932c-59eb4533043a",
   "metadata": {},
   "source": [
    "## Read from file (created with [this Colab notebook](https://colab.research.google.com/drive/19N8taRKeTt_Bgx_yF5AD5ntkU7zy7unc?usp=sharing)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d76a66-d52e-440d-a2c5-b74a1bf0d3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read list of dictionaries from a JSON file\n",
    "with open(Path(settings_dict['save_dir']).parent.parent / 'Ela/dictionary_jsons/dictionaries.json', 'r') as json_file: #assumes your save_dir is /mg_network/Rachel/ on your working computer\n",
    "    dict_all = json.load(json_file)\n",
    "  \n",
    "dict_list = dict_all.keys()\n",
    "\n",
    "# Convert lists to sets within each dictionary in data\n",
    "for key, dictionary in dict_all.items():\n",
    "    for sub_key, value in dictionary.items():\n",
    "        if isinstance(value, list):\n",
    "            dictionary[sub_key] = set(value)\n",
    "\n",
    "# Dynamically create dictionaries based on the JSON keys\n",
    "for key, value in dict_all.items():\n",
    "    globals()[key] = value\n",
    "\n",
    "print(dict_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f390b924-d9f5-450b-8f4d-7e9df4fca938",
   "metadata": {},
   "source": [
    "## Create locally\n",
    "\n",
    "On some computers this can take a long time (if low RAM or low memory available. If this takes more than 5 minutes, just use Colab notebook to do it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "beb9b8a5-66a0-4325-bbad-a5cf8764f832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create base segment dictionaries to check for duplicates\n",
    "# use manually-defined function in setup for \"get_base_segments_dict, which can check for files with same id)\"\n",
    "\n",
    "base_segments_Ela = get_base_segments_dict(Path(settings_dict['save_dir']).parent.parent / 'Ela/reconstructions')\n",
    "base_segments_main = get_base_segments_dict(Path(settings_dict['save_dir']))\n",
    "base_segments_todo = get_base_segments_dict(Path(settings_dict['save_dir']).parent.parent / 'Ela/PE_post')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256ae39b-a5c2-4b4b-a31b-e72bcde353eb",
   "metadata": {},
   "source": [
    "# Proofread using (e)CREST\n",
    "\n",
    "The ```ecrest``` class defined in eCREST_cli.py can be used to proofread base_segment reconstructions enhanced by the agglomeration database.\n",
    "\n",
    "An instance of this class can be initialized with either:\n",
    "- ecrest(segment_id): a \"main_base_id\" in *int* format\n",
    "- ecrest(filepath): an existing CREST .json file\n",
    "- ecrest(segment_id, segment_list): the main_base_id from the neuroglancer file you are converting and a list of base_segments.\n",
    "\n",
    "The ```launch_viewer``` flag default is \"False\" so that you can interact with the contents of a reconstruction without actually opening it visually in a neuroglancer tab. **NOTE**: Some ecrest functions require that the ecrest instance is created with ```launch_viewer==True```."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1bfe5c-959c-45ee-81c8-f0e1369bbe20",
   "metadata": {},
   "source": [
    "## NEW reconstruction from segment ID\n",
    "\n",
    "If you wanted to start reconstructing a new cell from a main base segment, you would use the following code block to launch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf77135f-3c40-42c3-bb22-df255dda90e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "segment_id = '199762907'\n",
    "crest = ecrest(settings_dict,segment_id = segment_id, launch_viewer=True)\n",
    "\n",
    "# The following is optional and can be commented out. It enables adding segments by alt+left-mouse instead of left-mouse double click. \n",
    "crest.change_key_binding({\"alt+mousedown0\" : \"add-or-remove-seg\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23c9d2e-6d4c-4c4d-ac6e-ff75d934946f",
   "metadata": {},
   "source": [
    "### Check for duplicates against main network, your todo file, and your reconstruction folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3555c8-4d66-4593-af6f-d39a8074f6c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check for duplicates of current reconstruction with base segments dictionary\n",
    "# to save time, this line can be run alone after initializing base_segments dictionary above\n",
    "print('overlap in Ela reconstruction folder:'); df = crest.check_duplicates(base_segments_Ela); display(df)\n",
    "print('overlap in main network folder:'); df = crest.check_duplicates(base_segments_main); display(df)\n",
    "print('overlap in todo folder:'); df = crest.check_duplicates(base_segments_todo); display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d1da8e-e4ee-448d-ad81-58d6549aca74",
   "metadata": {},
   "source": [
    "### Add soma annotations \n",
    "\n",
    "Only add soma annotations if it is a new reconstruction with a soma in the volume.  \n",
    "    - 4 annotations if the soma is fully in the volume, only 3 if partially out  \n",
    "    - annotations get added along the x plane and y plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a99a36-11f8-49c2-b141-340d0a784944",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "crest.add_endpoint_annotation_layers(['soma'],link=True) # spine_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb47596d-f326-4973-a347-ec556d5597ae",
   "metadata": {},
   "source": [
    "### define cell type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ec34a71d-a4af-43ba-a442-a79ca22ad4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_type = 'mg2' # Assign the cell type then run the code cell\n",
    "\n",
    "crest.define_ctype(cell_type,\"manual\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753a361a-2a10-4028-be75-2a9e54bd727a",
   "metadata": {},
   "source": [
    "### save reconstruction\n",
    "\n",
    "default location should be Ela_reconstructions (specified in settings_dict.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9019c017-dddb-48ba-8eb5-0a9747bf0466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cell 199762907 reconstruction locally at 2025-01-13 12.11.20\n"
     ]
    }
   ],
   "source": [
    "crest.save_cell_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279ae34a-a21c-48ed-96de-313dea636230",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "## EDIT reconstruction from file\n",
    "\n",
    "If you wanted to edit a reconstruction from an existing file, you would use the following code block to launch.\n",
    "\n",
    "Specify the cell_id and the path to the directory that cell is in. \n",
    "\n",
    "> NOTE: You can also directly copy paste the full filepath to the cell you want to open and pass it to the ```filepath``` flag.  \n",
    "In that case, the only code you need is crest = ecrest(settings_dict,filepath= [*paste filepath here*], launch_viewer=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f194c9bd-f490-4aa0-b1ba-29b7533f2a0e",
   "metadata": {},
   "source": [
    "### get reconstruction files in cell's directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "010eda02-c044-4008-8f76-9159a42d9f98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/kperks/Library/CloudStorage/GoogleDrive-sawtelllab@gmail.com/My Drive/ELL_connectome/CREST_reconstructions')"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if overlap is with the main netowork, then you want:\n",
    "Path(settings_dict['save_dir']).parent.parent / 'CREST_reconstructions/mg_network'\n",
    "\n",
    "# if overlap is with Ela reconstruction folder, then you want:\n",
    "Path(settings_dict['save_dir'])\n",
    "\n",
    "# if overlap is with todo folder, then open the todo folder cell and make sure that the blue segment is included in your reconstruction, then delete the other todo cell (or start from the other todo and include the one you are working on, etc):\n",
    "Path(settings_dict['save_dir']).parent / 'PE_post'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "561d668f-c91c-47ad-a9b1-b4ab7c4c5404",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = Path(settings_dict['save_dir'])\n",
    "\n",
    "cell_filepaths = get_cell_filepaths(directory_path) # gets filepaths for all cells in a directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9388dad6-db6c-453e-ad11-ace50c252c9a",
   "metadata": {},
   "source": [
    "### Open cell for adding todo segment(s) to existing reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c78001e-dcd5-4391-8d85-5e82241406f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cell_id = '387415866' # specify the cell id\n",
    "\n",
    "crest = ecrest(settings_dict,filepath= cell_filepaths[cell_id], launch_viewer=True)\n",
    "print(cell_filepaths[cell_id])\n",
    "\n",
    "# optional:\n",
    "crest.change_key_binding({\"alt+mousedown0\" : \"add-or-remove-seg\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac7a147-fff1-4921-a8b4-8190b741f05f",
   "metadata": {},
   "source": [
    "### Check for duplicates against main network, your todo file, and your reconstruction folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2c047c-73b5-4b53-89e6-d56b03a26562",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check for duplicates of current reconstruction with base segments dictionary\n",
    "# to save time, this line can be run alone after initializing base_segments dictionary above\n",
    "print('overlap in Ela reconstruction folder:'); df = crest.check_duplicates(base_segments_Ela); display(df)\n",
    "print('overlap in main network folder:'); df = crest.check_duplicates(base_segments_main); display(df)\n",
    "print('overlap in todo folder:'); df = crest.check_duplicates(base_segments_todo); display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b022d973-1166-47f5-9bba-b007435bbd7f",
   "metadata": {},
   "source": [
    "### SAVE edited reconstruction\n",
    "\n",
    "Save to default location (your reconstruction folder). so... if you later find a duplicate both with the main network and your reconstruction folder, keep adding on to the newer one in your reconstruction folder rather than going back to the original again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825865c2-34ee-417e-b461-8ef1bd078016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cell 299439953 reconstruction locally at 2025-01-14 20.20.34\n"
     ]
    }
   ],
   "source": [
    "crest.save_cell_graph() # Default location is Path(settings_dict['save_dir'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8caa4ab-7276-4b53-8c19-37f1fcd77bbb",
   "metadata": {},
   "source": [
    "# Get all postsynaptic PE partners in network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93601104-37bc-456d-b2bb-4669bb403b12",
   "metadata": {},
   "source": [
    "## dictionary all reconstruction files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ac9a1de-dca8-4011-880e-8590c56aa393",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirpath_list = [Path(settings_dict['save_dir']).parent.parent / 'CREST_reconstructions/mg_network', Path(settings_dict['save_dir'])]\n",
    "# if your directory is last in this list, then any newer versions that you have done should overwrite the older versions in the dictionary -- we can check this manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7c6b1ef-07ad-4e2b-8d5d-f4e00aeb789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirpath_list = [Path('/Users/kperks/Library/CloudStorage/GoogleDrive-sawtelllab@gmail.com/My Drive/ELL_connectome/CREST_reconstructions/mg-network'),Path('/Users/kperks/Library/CloudStorage/GoogleDrive-sawtelllab@gmail.com/My Drive/ELL_connectome/Ela/reconstructions')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "275f2e1e-3167-4545-ad73-48e0cb0b27c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nodefiles = {}\n",
    "for dirpath in dirpath_list:\n",
    "    d2 = get_cell_filepaths(dirpath)\n",
    "    nodefiles.update(d2) #The update() method overwrites the values of existing keys with the new values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1678b1cc-ba4f-4850-ba50-7d4cf1373194",
   "metadata": {},
   "source": [
    "## Base Segments of reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "670bc751-5388-43fe-877a-a61870c212ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nodefiles = get_cell_filepaths(dirpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c24f5e94-8508-4aea-8da1-e59b3106ee0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a base_segments dictionary of all reconstructed cells \n",
    "\n",
    "base_segments = {}\n",
    "for x,f in nodefiles.items():\n",
    "    # if cell_type[x] in network_types: # if do this, you can't check if the post-syn segments exist as a reconstruction\n",
    "    cell = ecrest(settings_dict,filepath = f)#,launch_viewer=False)\n",
    "    base_segments[cell.cell_data['metadata']['main_seg']['base']] = cell.cell_data['base_segments']\n",
    "    \n",
    "    try:\n",
    "        assert cell.cell_data['metadata']['main_seg']['base'] == x\n",
    "    except:\n",
    "        print(x,cell.cell_data['metadata']['main_seg']['base'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc270d24-8165-4da3-85bd-95fa75febee4",
   "metadata": {},
   "source": [
    "## Build synapses dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "677ddd94-bde2-495b-93a9-abbbe8ad6c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_type = pd.read_csv(dirpath.parent.parent / 'CREST_reconstructions/mg_network/metadata/df_type.csv')\n",
    "# df_type = pd.read_csv(Path('/Users/kperks/Library/CloudStorage/GoogleDrive-sawtelllab@gmail.com/My Drive/ELL_connectome/CREST_reconstructions/mg-network/metadata/df_type.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee3603e5-202b-47ae-a05f-6a010d17dfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_id_list = df_type[df_type['cell_type'].isin(['pe'])]['id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "021aeb89-a4a7-43ef-9853-8b583cc444e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 5705/5705 [02:54<00:00, 32.74it/s]\n"
     ]
    }
   ],
   "source": [
    "synanno_type = 'post-synaptic'\n",
    "vx_sizes = [16, 16, 30]\n",
    "\n",
    "## find edges and set the cell-structure attribute of the edge based on which part of the cell the edge goes to\n",
    "edge_list = []\n",
    "\n",
    "with tqdm(total=len(nodefiles.keys())) as pbar:\n",
    "    for x_pre in pe_id_list: #nodefiles.keys():\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # if df_type[df_type['id'].isin([int(x_pre)])]['cell_type'].values[0] in network_types:\n",
    "            \n",
    "        # if the node has post-synaptic annotations (the current cell is assumed pre-synaptic)\n",
    "        pre = ecrest(settings_dict,filepath = nodefiles[x_pre])#,launch_viewer=False)\n",
    "        if pre.cell_data['end_points'][synanno_type] != []:\n",
    "            # for each synapse\n",
    "            for syn_ in pre.cell_data['end_points'][synanno_type]:\n",
    "                '''assumes that the annotation is a point annotation stored in the list as ([x,y,z,segment_id],'annotatePoint')\n",
    "                previous to Jan 25 2024, it was just [x,y,z,segment_id]'''\n",
    "                syn_ = syn_[0]\n",
    "                try:\n",
    "                    post_seg = syn_[3]\n",
    "                    syn_ = array([int(syn_[i]) for i in range(3)]) # synapses annotations exported as nanometers, so do not need to convert\n",
    "\n",
    "                    # go through each other nodes\n",
    "                    for x_post in nodefiles.keys():\n",
    "                        # if cell_type[x_post] in network_types:\n",
    "                        post = base_segments[x_post] \n",
    "                        for k,v in post.items():\n",
    "                            for v_ in list(v): #find keys (can be multiple on the same cell) for matching segment ids\n",
    "                                if post_seg == v_: \n",
    "                                    # add edge to the graph between current node and matching node\n",
    "                                    edge_list.append([x_pre,x_post,k,syn_[0],syn_[1],syn_[2]])\n",
    "                                        \n",
    "\n",
    "                except IndexError as msg:\n",
    "                    cellid = x_pre\n",
    "                    print(msg, f'for cell {cellid} synapse at {array([int(syn_[i]/vx_sizes[i]) for i in range(3)])} voxels has no segment id')\n",
    "\n",
    "        else:\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c1371b-a1c1-4aac-8bed-36d2f4ec4621",
   "metadata": {},
   "source": [
    "## Synapses dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "451dbc73-b542-4e62-9102-fecfa1cc9554",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_syn = pd.DataFrame(edge_list,columns = ['pre','post','structure','x','y','z'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ed0d9f-aae0-4778-8f53-b124fde2538b",
   "metadata": {},
   "source": [
    "## SAVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b68a0b2d-793f-4cc6-9e43-652e1687aeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath = Path(Path(settings_dict['save_dir']).parent)\n",
    "\n",
    "df_syn.to_csv(savepath / 'df_pe_postsyn.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491273bc-960f-4f47-b2d1-f63a29b844ea",
   "metadata": {},
   "source": [
    "# Cell Types combined\n",
    "\n",
    "combine cell types from your reconstruction folder with types in main network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b11cb43-e177-4533-a3c9-79bde78b8cd3",
   "metadata": {},
   "source": [
    "## get cell types from reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1880f1f-183b-4575-92d2-4c9fd6c5e388",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dirpath = Path(settings_dict['save_dir'])\n",
    "nodefiles_Ela = get_cell_filepaths(dirpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ee6bc7-251f-4bdc-a0ee-3aa9042e1864",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_type = {}\n",
    "not_typed = []\n",
    "for x,f in nodefiles_Ela.items():\n",
    "    cell = ecrest(settings_dict,filepath = f,launch_viewer=False)\n",
    "    cell_type[int(x)] = cell.get_ctype('manual') \n",
    "    if (cell.get_ctype('manual') == []) | (cell.get_ctype('manual') == ''):\n",
    "        cell_type[int(x)]=np.NaN\n",
    "        not_typed.append(x)# print(f'cell {x} is not cell-typed in json')\n",
    "        \n",
    "print('the following cells are not typed in the main network')\n",
    "print(not_typed)        \n",
    "        \n",
    "df_type_Ela = pd.DataFrame(cell_type.items(),columns = ['id','cell_type'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e679d4-ba74-4ae8-9f9b-30c1c7fe0894",
   "metadata": {},
   "source": [
    "## get cell types from saved main network df_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51186037-99d0-4a94-831d-bf407cfce72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_type_main = pd.read_csv(dirpath.parent.parent / 'CREST_reconstructions/mg_network/metadata/df_type.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999bd6d7-bfd1-4f75-8e5e-2de494b56884",
   "metadata": {},
   "source": [
    "## combine df_type from Ela and main network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dd58aa-aa6b-4bc5-9c23-e55aa9924efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_type = pd.concat([df_type_Ela, df_type_main],ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e255a87-7198-48e3-b511-2c6f8cf6924f",
   "metadata": {},
   "source": [
    "# PE postsynaptic network analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "471f927b-1b24-48ef-bc14-94c1ba4b36a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_syn = pd.read_csv(Path(Path(settings_dict['save_dir']).parent) / 'df_pe_postsyn.csv')\n",
    "syn = 'post-synaptic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ba0058d0-5450-4827-96a2-9fde29435207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1069"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_syn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5128ad-810f-4343-8b02-2f2c93bf3346",
   "metadata": {},
   "source": [
    "## add cell type to df_syn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "daee1f32-7f4b-4b10-9be1-a20b4e0cc916",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,r in df_syn.iterrows():\n",
    "    try:\n",
    "        df_syn.loc[i,'pre_type'] =df_type[df_type['id'].isin([r['pre']])].cell_type.values[0]\n",
    "        df_syn.loc[i,'post_type']=df_type[df_type['id'].isin([r['post']])].cell_type.values[0]\n",
    "    except:\n",
    "        print(r['pre'],r['post'])\n",
    "        continue\n",
    "\n",
    "df_syn.loc[:,'post_type'] = [t.lower() for t in df_syn['post_type']]\n",
    "df_syn.loc[:,'pre_type'] = [t.lower() for t in df_syn['pre_type']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316686fd-41d9-47b5-8514-82e66ff71068",
   "metadata": {},
   "source": [
    "## Connection weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "259a2eca-80d7-423c-a6bc-307655124387",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edges = df_syn[['pre','post','pre_type','post_type']].value_counts().reset_index(name='weight') #'structure',"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0612a36a-fa2f-43da-b20e-8b9715248152",
   "metadata": {},
   "source": [
    "## Connection patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "d1e2c27e-2a46-4570-a42f-82168d01ad57",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_type = 'nsyn' # 'ncells' #\n",
    "mask = (df_edges['post_type'].isin(['sg1','sg2','mg1','mg2','lg','lf','mli','grc']))#,'smpl','mli','tsd','h'])) # df_edges['pre'].isin([290552453,27220895,31694533,102463116,188296613,15401313,17877032,187151336,117041378,122039969,36165549]) & \n",
    "#[295969348,295969442,295969134,295969355,295968777,282228761,283375247, 283391297,283390956,282230475,268614458,268614383,273086215,187230424]\n",
    "\n",
    "types_ = ['smpl'] #should be all that is in df_pfsyn anyway\n",
    "df_map = pd.DataFrame()\n",
    "for t in types_:\n",
    "    if count_type == 'nsyn':\n",
    "        df_grouped = df_edges[(df_edges['pre_type']==t) & mask].groupby(\n",
    "            ['pre','pre_type','post_type']).sum(numeric_only=True).reset_index().pivot(\n",
    "            index='pre', columns='post_type', values='weight').fillna(0).reset_index()\n",
    "    \n",
    "    if count_type == 'ncells':\n",
    "        df_grouped = df_edges[(df_edges['pre_type']==t) & mask].groupby(\n",
    "            ['pre','pre_type','post_type']).count().reset_index().pivot(\n",
    "            index='pre', columns='post_type', values='post').fillna(0).reset_index()\n",
    "    \n",
    "    df_grouped['pre_type']=t\n",
    "    df_map = pd.concat([df_map,df_grouped])\n",
    "    \n",
    "df_map = df_map.fillna(0)\n",
    "df_map = df_map.set_index('pre')\n",
    "df_map = df_map.drop(['pre_type'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee31e21-49f6-49e4-bb16-6a228191585e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_map.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e4a9b4-ca80-45b6-a5d6-fde02c53295e",
   "metadata": {},
   "source": [
    "### connectivity heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "7f3e4065-95a8-420a-82fe-f58b4df4dcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_order = ['lg','mg1','sg1','lf','mg2','sg2','mli']#,'smpl','mli','h','tsd']\n",
    "df_map = df_map.loc[:,[t for t in target_order if t in df_map.columns.values]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "f98e47d4-450c-485d-915b-a911ec7de0fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "post_type\n",
       "lg       1.0\n",
       "mg1      8.0\n",
       "sg1      6.0\n",
       "lf     175.0\n",
       "mg2    162.0\n",
       "sg2    250.0\n",
       "mli     81.0\n",
       "grc     50.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_map.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "14d12e95-20a7-4205-b3aa-376b5a0165c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort1 = 'mg1'\n",
    "sort1_map = df_map[df_map[sort1]>0].sort_values([sort1],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "a1b39b3b-9d23-4905-801e-9723f1a461e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort2 = 'mg2'\n",
    "sort2_map = df_map[~df_map.index.isin(sort1_map.index)].sort_values([sort2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "52c0b173-5d27-4a9f-b351-27cf1c513548",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_map = df_map[(~df_map.index.isin(sort1_map.index)) & ~df_map.index.isin(sort2_map.index)]#.sort_values(['mg2'],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "5766606b-8228-470e-b3fd-20d94c728476",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_full = pd.concat([sort1_map,other_map,sort2_map])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368c5989-3fc6-422a-8b7c-dad0360962a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the heatmap\n",
    "fig, axes = plt.subplots(1, figsize=(5,10))\n",
    "sns.heatmap(data=sorted_full, annot=False, fmt=\"0.0f\", cmap=cmap, \n",
    "            ax=axes, vmin=1)  # vmin slightly above 0 to treat negative as \"under\"\n",
    "axes.set_title('total syn per pre cell')\n",
    "axes.set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ba0411-1769-41b6-8b85-9567ff292f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "# Create a stacked bar plot\n",
    "sorted_full.loc[:,target_order].plot.bar(stacked=True,color = syn_colors,ax=ax, width=0.9)\n",
    "# ax.set_xticks([])\n",
    "ax.legend(title='Postsynaptic \\n Type', bbox_to_anchor=(1.35, 1), loc='upper right')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4030df-6c96-44f7-bbe2-3187f4c8a9bd",
   "metadata": {},
   "source": [
    "## conditional OUTPUT analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ad41a46c-aba8-478a-97d8-2909147a67e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conditional_output(df_edges,normalize=False):\n",
    "    '''get p(connect)'''\n",
    "    df_map = df_edges.groupby(['pre','pre_type','post_type']).sum(numeric_only=True).reset_index().pivot(index='pre', columns='post_type', values='weight').fillna(0).reset_index().set_index('pre')\n",
    "\n",
    "    if normalize==True:\n",
    "        df_map = df_map.div(df_map.sum(axis=1),axis=0)\n",
    "    \n",
    "        '''group data'''\n",
    "        result = []\n",
    "        for g in df_map.columns:\n",
    "            result.append(list(df_map[(df_map[g] > 0.05)].mean().values))\n",
    "\n",
    "    if normalize==False:\n",
    "        '''group data'''\n",
    "        result = []\n",
    "        for g in df_map.columns:\n",
    "            result.append(list(df_map[(df_map[g] > 1)].mean().values))\n",
    "\n",
    "    order = df_map.columns\n",
    "        \n",
    "    return result,order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4f3324db-72c1-47e9-86de-bc9c3725e0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_groups = ['pe']\n",
    "post_groups = ['mg1','mg2','lg','lf','sg1','sg2','mli']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5441b4-da9c-4aa4-8793-ca136c831194",
   "metadata": {},
   "source": [
    "### shuffle result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7e9fbc0e-9229-413c-acfa-88351419f0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_shuff = []\n",
    "\n",
    "# Iterate the specified number of times\n",
    "for i in range(100):\n",
    "    df_syn_shuff = deepcopy(df_syn)\n",
    "    mask = df_syn_shuff['pre_type'].isin(pre_groups) & df_syn_shuff['post_type'].isin(post_groups)  # Filter out rows with post_type not in post_types_order and pre_tyep not in pre_types_order\n",
    "    df_syn_shuff = df_syn_shuff[mask]\n",
    "    # Shuffle the dataframe\n",
    "    # df_syn_rand.loc[:,['pre']] = df_syn_rand['pre'].sample(frac = 1).values ## *** this does not work unless you re-type the pre_type column after***\n",
    "    shuff_rows = df_syn_shuff[['post','x','y','z','y_adj','post_type']].sample(frac = 1)\n",
    "    df_syn_shuff.loc[:,['post']] = shuff_rows['post'].values\n",
    "    df_syn_shuff.loc[:,['x']] = shuff_rows['x'].values\n",
    "    df_syn_shuff.loc[:,['y']] = shuff_rows['y'].values\n",
    "    df_syn_shuff.loc[:,['z']] = shuff_rows['z'].values\n",
    "    df_syn_shuff.loc[:,['y_adj']] = shuff_rows['y_adj'].values\n",
    "    df_syn_shuff.loc[:,['post_type']] = shuff_rows['post_type'].values\n",
    "\n",
    "    df_edges_shuff=df_syn_shuff[['pre','post','pre_type','post_type']].value_counts().reset_index(name='weight')\n",
    "    #df_syn_shuff.drop(['Unnamed: 0','x','y','z','y_adj','structure'],axis=1).value_counts().reset_index(name='weight')\n",
    "\n",
    "    result_,order = get_conditional_output(df_edges_shuff,normalize=True)\n",
    "    \n",
    "    # Append the result as a row to the result_df\n",
    "    result_shuff.append(result_)\n",
    "\n",
    "result_shuff = np.asarray(result_shuff)\n",
    "\n",
    "u_mat = result_shuff.mean(axis=0)\n",
    "\n",
    "std_mat = result_shuff.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01997f30-7327-4660-adaa-9049a33e9056",
   "metadata": {},
   "source": [
    "### data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d7a689b2-8f4f-4f33-a48e-e1fdcc31b305",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_syn_data = deepcopy(df_syn)\n",
    "mask = df_syn_data['pre_type'].isin(pre_groups) & df_syn_data['post_type'].isin(post_groups)  # Filter out rows with post_type not in post_types_order and pre_tyep not in pre_types_order\n",
    "df_syn_data = df_syn_data[mask]\n",
    "\n",
    "df_edges_data=df_syn_data[['pre','post','pre_type','post_type']].value_counts().reset_index(name='weight')\n",
    "#df_syn_data.drop(['Unnamed: 0','x','y','z','structure'],axis=1).value_counts().reset_index(name='weight')\n",
    "\n",
    "result_data,order = get_conditional_output(df_edges_data,normalize=True)\n",
    "\n",
    "# Calculate the z-scores\n",
    "z_scores = (result_data - u_mat) / std_mat\n",
    "z_scores[np.isclose(std_mat, 0)] = 0  # Replace z-scores with 0 where std is 0 # Handle cases where std_2d is zero to avoid division by zero\n",
    "\n",
    "\n",
    "cond_input_mat = pd.DataFrame(z_scores,columns = order, index = order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74cceb5-74f8-4f24-93eb-86b8c7295431",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the desired order\n",
    "order = ['sg1','mg1','lg','sg2','mg2','lf','mli']\n",
    "\n",
    "# Reorder rows and columns\n",
    "df_reordered=cond_input_mat.reindex(index=order, columns=order)\n",
    "\n",
    "# Ensure the color range is centered around 0\n",
    "vmin = -max(abs(df_reordered.min().min()), abs(df_reordered.max().max()))\n",
    "vmax = -vmin\n",
    "\n",
    "sns.set_context(\"paper\",font_scale=1)\n",
    "hfig,ax = plt.subplots(1,figsize=(2,2))\n",
    "sns.heatmap(df_reordered,\n",
    "    cmap=\"RdBu_r\",  # Diverging colormap from red to blue\n",
    "    vmin=vmin,\n",
    "    vmax=vmax,\n",
    "    center=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2740a33-aef0-4e75-b202-bf2335579cda",
   "metadata": {},
   "source": [
    "### an example of shuffle result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b9c34b-0f45-421a-9664-90cc54502482",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_syn_shuff = deepcopy(df_syn)\n",
    "mask = df_syn_shuff['pre_type'].isin(included_groups) & df_syn_shuff['post_type'].isin(['sg1','mg1','lg','sg2','mg2','lf'])  # Filter out rows with post_type not in post_types_order and pre_tyep not in pre_types_order\n",
    "df_syn_shuff = df_syn_shuff[mask]\n",
    "# Shuffle the dataframe\n",
    "# df_syn_rand.loc[:,['pre']] = df_syn_rand['pre'].sample(frac = 1).values ## *** this does not work unless you re-type the pre_type column after***\n",
    "shuff_rows = df_syn_shuff[['pre','x','y','z','pre_type']].sample(frac = 1)\n",
    "df_syn_shuff.loc[:,['pre']] = shuff_rows['pre'].values\n",
    "df_syn_shuff.loc[:,['x']] = shuff_rows['x'].values\n",
    "df_syn_shuff.loc[:,['y']] = shuff_rows['y'].values\n",
    "df_syn_shuff.loc[:,['z']] = shuff_rows['z'].values\n",
    "df_syn_shuff.loc[:,['y_adj']] = shuff_rows['y_adj'].values\n",
    "df_syn_shuff.loc[:,['pre_type']] = shuff_rows['pre_type'].values\n",
    "\n",
    "df_edges_shuff=df_edges_shuff[['pre','post','pre_type','post_type']].value_counts().reset_index(name='weight')\n",
    "#df_syn_shuff.drop(['Unnamed: 0','x','y','z','y_adj','structure'],axis=1).value_counts().reset_index(name='weight')\n",
    "\n",
    "result_,order = get_conditional_output(df_edges_shuff)\n",
    "\n",
    "cond_input_mat = pd.DataFrame(result_,columns = df_map.columns, index = df_map.columns)\n",
    "\n",
    "# Define the desired order\n",
    "order = ['sg1','mg1','lg','sg2','mg2','lf']\n",
    "\n",
    "# Reorder rows and columns\n",
    "df_reordered=cond_input_mat.reindex(index=order, columns=order)\n",
    "\n",
    "# Ensure the color range is centered around 0\n",
    "vmin = -max(abs(df_reordered.min().min()), abs(df_reordered.max().max()))\n",
    "vmax = -vmin\n",
    "\n",
    "sns.heatmap(df_reordered,\n",
    "    cmap=\"RdBu_r\",  # Diverging colormap from red to blue\n",
    "    vmin=vmin,\n",
    "    vmax=vmax,\n",
    "    center=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb16de5-5532-46ae-96f0-4cad40a56706",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c49c3e99-043a-45eb-a602-0b4550e55f58",
   "metadata": {},
   "source": [
    "# TODO reconstruction files from synapses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75d7684f-ab5f-4652-8aaf-4a650dd89c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_to_find = set()       \n",
    "\n",
    "syn_type = 'post-synaptic'#'spine_inputs' #\n",
    "\n",
    "vx_sizes = [16, 16, 30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7b4ff3-7e27-48b1-b0a1-fc464e9a865d",
   "metadata": {},
   "source": [
    "## dictionary all reconstruction files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3a75039-7e96-4e8b-870a-960ff35961aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirpath_list = [Path(settings_dict['save_dir']).parent.parent / 'CREST_reconstructions/mg_network', Path(settings_dict['save_dir'])]\n",
    "# if your directory is last in this list, then any newer versions that you have done should overwrite the older versions in the dictionary -- we can check this manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20a8e58f-364d-40dc-86be-e120d6a7c1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirpath_list = [Path('/Users/kperks/Library/CloudStorage/GoogleDrive-sawtelllab@gmail.com/My Drive/ELL_connectome/CREST_reconstructions/mg-network'),Path('/Users/kperks/Library/CloudStorage/GoogleDrive-sawtelllab@gmail.com/My Drive/ELL_connectome/Ela/reconstructions')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9ddf69d-7b3b-437b-b592-dd074e471334",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nodefiles = {}\n",
    "for dirpath in dirpath_list:\n",
    "    d2 = get_cell_filepaths(dirpath)\n",
    "    nodefiles.update(d2) #The update() method overwrites the values of existing keys with the new values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19be68dd-4def-46e9-ac79-977ace5f0f41",
   "metadata": {},
   "source": [
    "## Base Segments of reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b751ebd4-c6c8-4a88-98da-ce363606d6cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nodefiles = get_cell_filepaths(dirpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66349d8-ed12-4f4f-851f-3e4a98af01ed",
   "metadata": {},
   "source": [
    "pfs reconstructed from each type\n",
    "299496636 mg1 5/22, \n",
    "214581797 mg2 1/22, \n",
    "301787806 lg, 11/11\n",
    "393325331 lf, 8/16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48a5d0b3-f2bd-4745-bc8c-b390cccd794f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cells_todo = ['447127759'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d54a1ce-0c1c-4796-bb9d-31a93d5ced25",
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_to_find = set()\n",
    "\n",
    "for c_id in cells_todo:\n",
    "    crest = ecrest(settings_dict,filepath= nodefiles[c_id], launch_viewer=False)\n",
    "    for syn_ in crest.cell_data['end_points'][syn_type]:\n",
    "        try:\n",
    "            syn_to_find.add(syn_[0][3])\n",
    "\n",
    "        except IndexError as msg:\n",
    "            cellid = crest.cell_data['metadata']['main_seg']['base']\n",
    "            print(msg, f'for cell {cellid} synapse at {array([int(syn_[0][i]/vx_sizes[i]) for i in range(3)])} has no segment id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "361c15bc-acb7-46cc-a8cc-b53043ddc831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(crest.cell_data['end_points'][syn_type])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "48fe1814-a606-420b-861b-4cfee8e7223d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(syn_to_find)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc283d3-5611-4aa8-8961-c9d69cd4d0ba",
   "metadata": {},
   "source": [
    "First, find if any of these post-synaptic segments are already part of reconstructions completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38c4aa02-db7f-4ffc-b2d7-71258949b9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a base_segments dictionary of all reconstructed cells \n",
    "# base_segments = crest.get_base_segments_dict(Path(settings_dict['save_dir']))\n",
    "\n",
    "base_segments = {}\n",
    "for x,f in nodefiles.items():\n",
    "    # if cell_type[x] in network_types: # if do this, you can't check if the post-syn segments exist as a reconstruction\n",
    "    cell = ecrest(settings_dict,filepath = f)#,launch_viewer=False)\n",
    "    base_segments[cell.cell_data['metadata']['main_seg']['base']] = cell.cell_data['base_segments']\n",
    "    \n",
    "    try:\n",
    "        assert cell.cell_data['metadata']['main_seg']['base'] == x\n",
    "    except:\n",
    "        print(x,cell.cell_data['metadata']['main_seg']['base'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0bf819ba-3bba-4740-9a73-688ff65f7020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topop=set()\n",
    "for k,v in base_segments.items():\n",
    "    if syn_to_find & v != set():\n",
    "        # print(f'use reconstruction {k}')\n",
    "        topop = topop.union(syn_to_find & v)\n",
    "    \n",
    "len(topop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff696e5-2482-4919-b8fb-248884f83177",
   "metadata": {},
   "source": [
    "Adjust \"syn_to_find\" to eliminate these base segments from the todo list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd0adaa6-dbd2-4806-a442-c05b0d75db8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn_to_find = syn_to_find.difference(topop)\n",
    "\n",
    "len(syn_to_find)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1ac5f8-b16a-4f56-b5bb-b1165757b87d",
   "metadata": {},
   "source": [
    "create crest files for each of the unidentified post-synaptic partners\n",
    "\n",
    "Save reconstructed_segs as a json to go through manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa63f84f-ebf6-4882-9fbb-c121d43f78cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "todo_folder_path = Path(settings_dict['save_dir']).parent / 'PE_post'\n",
    "\n",
    "for segment_id in sorted(list(syn_to_find)):\n",
    "\n",
    "    cell = ecrest(settings_dict,segment_id = segment_id, launch_viewer=False)\n",
    "    cell.save_cell_graph(directory_path = todo_folder_path)#'todo_presynaptic/Krista/sgx_394470350')#/Krista/mg_214581797')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
