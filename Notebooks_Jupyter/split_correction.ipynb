{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import json\n",
    "# Initialize a viewer with the base segmentation for the box of interest\n",
    "import neuroglancer\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_next_element(viewer,neurites):\n",
    "    element = int(neurites[0])\n",
    "    \n",
    "    # Define color mapping for the segment(s)\n",
    "    color_value = \"#ef8200\"\n",
    "    segment_color_mapping = {element: color_value}\n",
    "    \n",
    "    # Update the viewer\n",
    "    with viewer.txn() as s:\n",
    "        # Assign segments using `set()`, since `.segments` is a Neuroglancer `VisibleSegments` object\n",
    "        s.layers['segmentation'].segments = set([element])\n",
    "        s.layers['segmentation'].segment_colors = segment_color_mapping\n",
    "        s.layers['segmentation'].visible = True\n",
    "    \n",
    "    print(f'this element is {element}')\n",
    "    print('Remaining neurites '+ str(len(neurites)))\n",
    "    \n",
    "    return element\n",
    "\n",
    "def query_save_and_reset_viewer(viewer,element,proofread_objects,filename_proofread_list,filename_ref_list):\n",
    "    ##################################################################################################\n",
    "    # SAVE\n",
    "    \n",
    "    segments = [int(s) for s in list((viewer.state.layers['segmentation'].segments))]\n",
    "    \n",
    "    if str(element) in list(proofread_objects.keys()):\n",
    "        print('ERROR! element already used as a key in the json dict')\n",
    "        # Grab the reference list segments (all segments in the box >1000 voxels)\n",
    "        with open(filename_ref_list, 'r') as csv_file:\n",
    "            reader = csv.reader(csv_file)\n",
    "            ref_list = [int(value) for row in reader for value in row]  # Flattened list of uint64 values\n",
    "            proofread_list = set([num for k,v in proofread_objects.items() for num in v])\n",
    "            neurites = list(set(ref_list).difference(proofread_list))\n",
    "        return proofread_objects, neurites\n",
    "        \n",
    "    elif str(element) not in list(proofread_objects.keys()):\n",
    "        print('this element does not exist -- updating proofread_elements and reseting viewer')\n",
    "        proofread_objects[element] = segments # key need s to be element, because element is not always segments[0] and then end up with duplicate keys in split_corrected\n",
    "           \n",
    "    with open(filename_proofread_list, 'w') as outfile:\n",
    "        # json.dump([{k:v} for k,v in proofread_objects.items()], outfile) #use this if want a list of dictionaries rather than a dictionary of all\n",
    "        json.dump(proofread_objects, outfile)\n",
    "    \n",
    "    ################################################################################################\n",
    "    # RESET \n",
    "    # Grab the reference list segments (all segments in the box >1000 voxels)\n",
    "    with open(filename_ref_list, 'r') as csv_file:\n",
    "        reader = csv.reader(csv_file)\n",
    "        ref_list = [int(value) for row in reader for value in row]  # Flattened list of uint64 values\n",
    "    \n",
    "    # Grab the split-corrected segments so far\n",
    "    if os.path.exists(filename_proofread_list):\n",
    "        with open(filename_proofread_list, 'r') as f:\n",
    "            proofread_objects = json.load(f)\n",
    "    else:\n",
    "        with open(filename_proofread_list, 'w') as outfile:  \n",
    "            json.dump([], outfile)\n",
    "        proofread_objects = {}\n",
    "    \n",
    "    # Extract proofread segment IDs (make it a flat list) and subtract it from the ref_list to get the remaining list\n",
    "    proofread_list = set([num for k,v in proofread_objects.items() for num in v])\n",
    "    neurites = list(set(ref_list).difference(proofread_list))\n",
    "    ntotal = len(neurites)\n",
    "    print('Remaining neurites '+ str(len(neurites)))\n",
    "    \n",
    "    # Remove all segments from the \"segmentation\" layer\n",
    "    with viewer.txn() as txn:\n",
    "        layer = txn.layers['segmentation']\n",
    "        layer.segments = set()  # Assign an empty set to clear the segments\n",
    "    \n",
    "    with viewer.txn(overwrite=True) as s:\n",
    "        s.layers['Base Segment Merger'] = neuroglancer.AnnotationLayer()\n",
    "        s.layers['Base Segment Merger'].filterBySegmentation = [\"segments\"]\n",
    "        s.layers['Base Segment Merger'].linkedSegmentationLayer = {\"segments\": 'segmentation'}\n",
    "        s.layers['Base Segment Merger'].annotationColor = '#ffa500'\n",
    "        s.layers['Base Segment Merger'].tool = \"annotatePoint\"\n",
    "        \n",
    "    \n",
    "    point_type = 'ends'\n",
    "    with viewer.txn(overwrite=True) as s:\n",
    "        s.layers[point_type] = neuroglancer.AnnotationLayer()\n",
    "        s.layers[point_type].annotationColor = '#ffff00'\n",
    "        s.layers[point_type].tool = \"annotatePoint\"\n",
    "        s.layers[point_type].tab = 'Annotations'\n",
    "    \n",
    "    # RUN STEP1 again\n",
    "    \n",
    "    return proofread_objects, neurites\n",
    "\n",
    "def save_and_reset_viewer(viewer,proofread_objects,filename_proofread_list,filename_ref_list):\n",
    "    ##################################################################################################\n",
    "    # SAVE\n",
    "\n",
    "    with open(filename_proofread_list, 'w') as outfile:\n",
    "        # json.dump([{k:v} for k,v in proofread_objects.items()], outfile) #use this if want a list of dictionaries rather than a dictionary of all\n",
    "        json.dump(proofread_objects, outfile)\n",
    "    \n",
    "    ################################################################################################\n",
    "    # RESET \n",
    "    # Grab the reference list segments (all segments in the box >1000 voxels)\n",
    "    with open(filename_ref_list, 'r') as csv_file:\n",
    "        reader = csv.reader(csv_file)\n",
    "        ref_list = [int(value) for row in reader for value in row]  # Flattened list of uint64 values\n",
    "    \n",
    "    # Grab the split-corrected segments so far\n",
    "    if os.path.exists(filename_proofread_list):\n",
    "        with open(filename_proofread_list, 'r') as f:\n",
    "            proofread_objects = json.load(f)\n",
    "    else:\n",
    "        with open(filename_proofread_list, 'w') as outfile:  \n",
    "            json.dump([], outfile)\n",
    "        proofread_objects = {}\n",
    "    \n",
    "    # Extract proofread segment IDs (make it a flat list) and subtract it from the ref_list to get the remaining list\n",
    "    proofread_list = set([num for k,v in proofread_objects.items() for num in v])\n",
    "    neurites = list(set(ref_list).difference(proofread_list))\n",
    "    ntotal = len(neurites)\n",
    "    print('Remaining neurites '+ str(len(neurites)))\n",
    "    \n",
    "    # Remove all segments from the \"segmentation\" layer\n",
    "    with viewer.txn() as txn:\n",
    "        layer = txn.layers['segmentation']\n",
    "        layer.segments = set()  # Assign an empty set to clear the segments\n",
    "    \n",
    "    with viewer.txn(overwrite=True) as s:\n",
    "        s.layers['Base Segment Merger'] = neuroglancer.AnnotationLayer()\n",
    "        s.layers['Base Segment Merger'].filterBySegmentation = [\"segments\"]\n",
    "        s.layers['Base Segment Merger'].linkedSegmentationLayer = {\"segments\": 'segmentation'}\n",
    "        s.layers['Base Segment Merger'].annotationColor = '#ffa500'\n",
    "        s.layers['Base Segment Merger'].tool = \"annotatePoint\"\n",
    "        \n",
    "    \n",
    "    point_type = 'ends'\n",
    "    with viewer.txn(overwrite=True) as s:\n",
    "        s.layers[point_type] = neuroglancer.AnnotationLayer()\n",
    "        s.layers[point_type].annotationColor = '#ffff00'\n",
    "        s.layers[point_type].tool = \"annotatePoint\"\n",
    "        s.layers[point_type].tab = 'Annotations'\n",
    "    \n",
    "    # RUN STEP1 again\n",
    "    \n",
    "    return proofread_objects, neurites\n",
    "    \n",
    "def check_against_proofread(segments,proofread_objects):\n",
    "    overlap = []\n",
    "    num_dup = []\n",
    "    for k,v in proofread_objects.items():\n",
    "        overlap.append(len(set(self_segments)&set(v))/len(set(v)))\n",
    "        num_dup.append(len(set(self_segments)&set(v)))\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        \"dups\": list(proofread_objects.keys()),\n",
    "        \"overlap-percent\": overlap,\n",
    "        \"number_seg_lap\": num_dup\n",
    "        }).replace(0, np.nan, inplace=False).dropna()\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining neurites 2220\n"
     ]
    }
   ],
   "source": [
    "# INITIALIZE #\n",
    "dirpath = '/Users/kperks/Library/CloudStorage/GoogleDrive-sawtelllab@gmail.com/My Drive/ELL_connectome/MolecularLayerModel_Michal/Efish_proofreading_box1/'\n",
    "filename_ref_list = dirpath + \"efish_box1_segments_list.csv\"\n",
    "# filename_proofread_list = 'box1_split_corrected.json'\n",
    "filename_proofread_list = dirpath + 'box1_split_corrected_dict.json'\n",
    "\n",
    "# Grab the reference list segments (all segments in the box >1000 voxels)\n",
    "with open(filename_ref_list, 'r') as csv_file:\n",
    "    reader = csv.reader(csv_file)\n",
    "    ref_list = [int(value) for row in reader for value in row]  # Flattened list of uint64 values\n",
    "\n",
    "# Grab the split-corrected segments so far\n",
    "if os.path.exists(filename_proofread_list):\n",
    "    with open(filename_proofread_list, 'r') as f:\n",
    "        proofread_objects = json.load(f)\n",
    "else:\n",
    "    with open(filename_proofread_list, 'w') as outfile:  \n",
    "        json.dump([], outfile)\n",
    "    proofread_objects = []\n",
    "\n",
    "# Extract proofread segment IDs (make it a flat list) and subtract it from the ref_list to get the remaining list\n",
    "proofread_list = set([num for k,v in proofread_objects.items() for num in v])\n",
    "remaining_list = list(set(ref_list).difference(proofread_list))\n",
    "neurites = remaining_list\n",
    "ntotal = len(neurites)\n",
    "print('Remaining neurites '+ str(len(neurites)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"http://127.0.0.1:53514/v/0752230f86399171d49f091744a5feabba5afb22/\" target=\"_blank\">Viewer</a>"
      ],
      "text/plain": [
       "http://127.0.0.1:53514/v/0752230f86399171d49f091744a5feabba5afb22/"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viewer = neuroglancer.Viewer()\n",
    "with viewer.txn() as s:\n",
    "    s.layers['image'] = neuroglancer.ImageLayer(source='brainmaps://10393113184:ell:roi450um_xyz')\n",
    "    s.layers['segmentation'] = neuroglancer.SegmentationLayer(\n",
    "        source='brainmaps://10393113184:ell:roi450um_seg32fb16fb_220930', \n",
    "        selected_alpha=0.5\n",
    "    )\n",
    "\n",
    "# Initialize a neurites counter\n",
    "viewer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{\n",
    "  \"type\": \"annotation\",\n",
    "  \"source\": {\n",
    "    \"url\": \"local://annotations\",\n",
    "    \"transform\": {\n",
    "      \"outputDimensions\": {\n",
    "        \"x\": [\n",
    "          1.6e-8,\n",
    "          \"m\"\n",
    "        ],\n",
    "        \"y\": [\n",
    "          1.6e-8,\n",
    "          \"m\"\n",
    "        ],\n",
    "        \"z\": [\n",
    "          3e-8,\n",
    "          \"m\"\n",
    "        ]\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"tool\": \"annotateBoundingBox\",\n",
    "  \"tab\": \"annotations\",\n",
    "  \"annotations\": [\n",
    "    {\n",
    "      \"pointA\": [\n",
    "        15075,\n",
    "        10620,\n",
    "        1618\n",
    "      ],\n",
    "      \"pointB\": [\n",
    "        15700,\n",
    "        11245,\n",
    "        1951\n",
    "      ],\n",
    "      \"type\": \"axis_aligned_bounding_box\",\n",
    "      \"id\": \"8470042ac60231441cf2de9988aabfc42cfe1965\"\n",
    "    }\n",
    "  ],\n",
    "  \"name\": \"box1\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with viewer.txn(overwrite=True) as s:\n",
    "\n",
    "    s.layers['Base Segment Merger'] = neuroglancer.AnnotationLayer()\n",
    "    s.layers['Base Segment Merger'].filterBySegmentation = [\"segments\"]\n",
    "    s.layers['Base Segment Merger'].linkedSegmentationLayer = {\"segments\": 'segmentation'}\n",
    "    s.layers['Base Segment Merger'].annotationColor = '#ffa500'\n",
    "    s.layers['Base Segment Merger'].tool = \"annotatePoint\"\n",
    "    \n",
    "\n",
    "point_type = 'ends'\n",
    "with viewer.txn(overwrite=True) as s:\n",
    "    s.layers[point_type] = neuroglancer.AnnotationLayer()\n",
    "    s.layers[point_type].annotationColor = '#ffff00'\n",
    "    s.layers[point_type].tool = \"annotatePoint\"\n",
    "    s.layers[point_type].tab = 'Annotations'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this element is 286796772\n",
      "Remaining neurites 1987\n"
     ]
    }
   ],
   "source": [
    "# Load the first element of the neurites list and make sure it is an int\n",
    "element = load_next_element(viewer,neurites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dups</th>\n",
       "      <th>overlap-percent</th>\n",
       "      <th>number_seg_lap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [dups, overlap-percent, number_seg_lap]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check to see if duplicate with any already read proofread objects\n",
    "self_segments = [int(s) for s in list((viewer.state.layers['segmentation'].segments))]\n",
    "df_dups = check_against_proofread(self_segments,proofread_objects)\n",
    "display(df_dups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this element does not exist -- updating proofread_elements and reseting viewer\n",
      "Remaining neurites 1987\n"
     ]
    }
   ],
   "source": [
    "proofread_objects, neurites = query_save_and_reset_viewer(viewer,element,proofread_objects,filename_proofread_list,filename_ref_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# consolidate with existing duplicate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## if you are sure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dupid = '286796800'\n",
    "consolidated_list = set(proofread_objects[dupid] + self_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "proofread_objects[dupid] = list(consolidated_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining neurites 2005\n"
     ]
    }
   ],
   "source": [
    "proofread_objects, neurites = save_and_reset_viewer(viewer,proofread_objects,filename_proofread_list,filename_ref_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## if you want to check visually first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the viewer\n",
    "lname = 'self'\n",
    "segment_color_mapping = {key: \"#ef8200\" for key in self_segments}\n",
    "with viewer2.txn() as s:\n",
    "    s.layers[lname] = neuroglancer.SegmentationLayer(\n",
    "        source='brainmaps://10393113184:ell:roi450um_seg32fb16fb_220930', \n",
    "        selected_alpha=0.5)\n",
    "    # Assign segments using `set()`, since `.segments` is a Neuroglancer `VisibleSegments` object\n",
    "    s.layers[lname].segments = set(self_segments)\n",
    "    s.layers[lname].segment_colors = segment_color_mapping\n",
    "    s.layers[lname].visible = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "for d in df_dups['dups'].values:\n",
    "    segs = list(set(proofread_objects[d]))\n",
    "    # color_value = generate_random_hex_color()# \"#ef8200\"\n",
    "    # segment_color_mapping = {key: generate_random_hex_color() for key in segs}\n",
    "    \n",
    "    \n",
    "    # Update the viewer\n",
    "    lname = d\n",
    "    with viewer2.txn() as s:\n",
    "        s.layers[lname] = neuroglancer.SegmentationLayer(\n",
    "            source='brainmaps://10393113184:ell:roi450um_seg32fb16fb_220930', \n",
    "            selected_alpha=0.5)\n",
    "        # Assign segments using `set()`, since `.segments` is a Neuroglancer `VisibleSegments` object\n",
    "        s.layers[lname].segments = set(segs)\n",
    "        # s.layers[lname].segment_colors = segment_color_mapping\n",
    "        s.layers[lname].visible = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dupid = '286796849'\n",
    "consolidated_list = set(proofread_objects[dupid] + self_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lname = 'consolidated'\n",
    "segment_color_mapping = {key: \"#ef8200\" for key in self_segments}\n",
    "with viewer2.txn() as s:\n",
    "    s.layers[lname] = neuroglancer.SegmentationLayer(\n",
    "        source='brainmaps://10393113184:ell:roi450um_seg32fb16fb_220930', \n",
    "        selected_alpha=0.5)\n",
    "    # Assign segments using `set()`, since `.segments` is a Neuroglancer `VisibleSegments` object\n",
    "    s.layers[lname].segments = set(consolidated_list)\n",
    "    # s.layers[lname].segment_colors = segment_color_mapping\n",
    "    s.layers[lname].visible = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1782,
   "metadata": {},
   "outputs": [],
   "source": [
    "proofread_objects[dupid] = list(consolidated_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1783,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining neurites 3051\n"
     ]
    }
   ],
   "source": [
    "proofread_objects, neurites = save_and_reset_viewer(viewer,proofread_objects,filename_proofread_list,filename_ref_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2054,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filename_proofread_list, 'r') as f:\n",
    "    proofread_objects = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicates(base_segments):\n",
    "    '''\n",
    "    base_segments is a dictionary of all segments that this script checks among\n",
    "    '''\n",
    "    df_all = pd.DataFrame()\n",
    "    dups_list = []\n",
    "    for self_k,this_cell in base_segments.items():\n",
    "        # print(dups_list)\n",
    "        if self_k not in dups_list:\n",
    "            overlap = []\n",
    "            num_dup = []\n",
    "            for x in base_segments.keys():\n",
    "                overlap.append(len(set(this_cell)&set(base_segments[x]))/len(set(base_segments[x])))\n",
    "                num_dup.append(len(set(this_cell)&set(base_segments[x])))\n",
    "    \n",
    "            df = pd.DataFrame({\n",
    "                \"self\": self_k,\n",
    "                \"dups\": list(base_segments.keys()),\n",
    "                \"overlap-percent\": overlap,\n",
    "                \"number_seg_lap\": num_dup\n",
    "                }).replace(0, np.nan, inplace=False).dropna()\n",
    "            df = df[df['dups'] != self_k]\n",
    "            \n",
    "            if not df.empty:\n",
    "                dups_list.extend([v for v in df['dups'].values])\n",
    "                df_all = pd.concat([df_all,df]) \n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"http://127.0.0.1:53514/v/a06100f4f3b2c581b7d188f908c00728ca8aba76/\" target=\"_blank\">Viewer</a>"
      ],
      "text/plain": [
       "http://127.0.0.1:53514/v/a06100f4f3b2c581b7d188f908c00728ca8aba76/"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viewer2 = neuroglancer.Viewer()\n",
    "with viewer2.txn() as s:\n",
    "    s.layers['image'] = neuroglancer.ImageLayer(source='brainmaps://10393113184:ell:roi450um_xyz')\n",
    "\n",
    "# Initialize a neurites counter\n",
    "viewer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2523,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_dict = check_duplicates(proofread_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2524,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>self</th>\n",
       "      <th>dups</th>\n",
       "      <th>overlap-percent</th>\n",
       "      <th>number_seg_lap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>371507331</td>\n",
       "      <td>371508228</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>372667908</td>\n",
       "      <td>372654163</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>285652483</td>\n",
       "      <td>285637634</td>\n",
       "      <td>0.004357</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>371507421</td>\n",
       "      <td>285637634</td>\n",
       "      <td>0.004357</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>654</th>\n",
       "      <td>371508122</td>\n",
       "      <td>371509018</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          self       dups  overlap-percent  number_seg_lap\n",
       "596  371507331  371508228         0.400000             2.0\n",
       "412  372667908  372654163         0.200000             2.0\n",
       "25   285652483  285637634         0.004357             2.0\n",
       "25   371507421  285637634         0.004357             2.0\n",
       "654  371508122  371509018         0.833333             5.0"
      ]
     },
     "execution_count": 2524,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dup_dict[dup_dict['number_seg_lap']>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2325,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proofread_objects.pop('371523785')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2525,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_id = '371508122'\n",
    "dup_list = list(dup_dict[(dup_dict['self']==self_id) & (dup_dict['number_seg_lap']>1)]['dups'].values)\n",
    "\n",
    "for d in [self_id] + dup_list:\n",
    "    # Define color mapping for the segment(s)\n",
    "    segs = list(set(proofread_objects[d]))\n",
    "    color_value = generate_random_hex_color()# \"#ef8200\"\n",
    "    segment_color_mapping = {key: color_value for key in segs}\n",
    "\n",
    "\n",
    "    # Update the viewer\n",
    "    lname = d\n",
    "    with viewer2.txn() as s:\n",
    "        s.layers[lname] = neuroglancer.SegmentationLayer(\n",
    "            source='brainmaps://10393113184:ell:roi450um_seg32fb16fb_220930', \n",
    "            selected_alpha=0.5)\n",
    "        # Assign segments using `set()`, since `.segments` is a Neuroglancer `VisibleSegments` object\n",
    "        s.layers[lname].segments = segs\n",
    "        s.layers[lname].segment_colors = segment_color_mapping\n",
    "        s.layers[lname].visible = True\n",
    "    \n",
    "    # print(f'this element is {element}')\n",
    "    # print('Remaining neurites '+ str(len(neurites)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2526,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_dict_consolidated = {}\n",
    "\n",
    "v_list = []\n",
    "for d in [self_id] + dup_list:\n",
    "    v_list.append(proofread_objects[d])\n",
    "dup_dict_consolidated[self_id] = list(set([num for ni in v_list for num in ni]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2527,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dup_dict_consolidated\n",
    "\n",
    "segs = dup_dict_consolidated[self_id]\n",
    "color_value = generate_random_hex_color()# \"#ef8200\"\n",
    "segment_color_mapping = {key: color_value for key in segs}\n",
    "\n",
    "\n",
    "# Update the viewer\n",
    "lname = f'{self_id}_consolidated'\n",
    "with viewer2.txn() as s:\n",
    "    s.layers[lname] = neuroglancer.SegmentationLayer(\n",
    "        source='brainmaps://10393113184:ell:roi450um_seg32fb16fb_220930', \n",
    "        selected_alpha=0.5)\n",
    "    # Assign segments using `set()`, since `.segments` is a Neuroglancer `VisibleSegments` object\n",
    "    s.layers[lname].segments = set(segs)\n",
    "    s.layers[lname].segment_colors = segment_color_mapping\n",
    "    s.layers[lname].visible = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2528,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['371509018']"
      ]
     },
     "execution_count": 2528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dup_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2529,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove keys using dictionary comprehension\n",
    "proofread_objects = {k: v for k, v in proofread_objects.items() if k not in dup_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2530,
   "metadata": {},
   "outputs": [],
   "source": [
    "proofread_objects[self_id] = dup_dict_consolidated[self_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2531,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dup_dict_consolidated\n",
    "\n",
    "segs = proofread_objects[self_id]\n",
    "color_value = generate_random_hex_color()# \"#ef8200\"\n",
    "segment_color_mapping = {key: color_value for key in segs}\n",
    "\n",
    "\n",
    "# Update the viewer\n",
    "lname = f'{self_id}_consolidated_final'\n",
    "with viewer2.txn() as s:\n",
    "    s.layers[lname] = neuroglancer.SegmentationLayer(\n",
    "        source='brainmaps://10393113184:ell:roi450um_seg32fb16fb_220930', \n",
    "        selected_alpha=0.5)\n",
    "    # Assign segments using `set()`, since `.segments` is a Neuroglancer `VisibleSegments` object\n",
    "    s.layers[lname].segments = set(segs)\n",
    "    s.layers[lname].segment_colors = segment_color_mapping\n",
    "    s.layers[lname].visible = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save proofread_objects if corrected now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2532,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filename_proofread_list, 'w') as outfile:\n",
    "    # json.dump([{k:v} for k,v in proofread_objects.items()], outfile) #use this if want a list of dictionaries rather than a dictionary of all\n",
    "    json.dump(proofread_objects, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# combine dicts with same keys (or re-key them)\n",
    "\n",
    "only duplicates where overlap is not in the base segment merge list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import random\n",
    "\n",
    "def generate_random_hex_color():\n",
    "  \"\"\"Generates a random hex color string.\"\"\"\n",
    "  hex_color = '#' + ''.join([random.choice('0123456789abcdef') for j in range(6)])\n",
    "  return hex_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1025,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#2363ea\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "random_color = generate_random_hex_color()\n",
    "print(random_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1039,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filename_proofread_list, 'r') as f:\n",
    "    proofread_objects = json.load(f)\n",
    "    # proofread_objects = {k: v for l in proofread_objects for k, v in l.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1040,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_list = [k for l in proofread_objects for k, v in l.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1041,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = [item for item, count in Counter(key_list).items() if count > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1042,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['371523882', '285637634', '372669442', '371507724', '371523648']"
      ]
     },
     "execution_count": 1042,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1043,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_dict = {}\n",
    "for d in duplicates:\n",
    "    v_list = []\n",
    "    for i,l in enumerate(proofread_objects):\n",
    "        k = list(l.keys())[0]\n",
    "        if k == d:\n",
    "            v_list.append(l[k])\n",
    "    dup_dict[d] = [num for ni in v_list for num in ni]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1009,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"http://127.0.0.1:58455/v/9d16af3d4b925d1014b2a86b150b63f39a39731e/\" target=\"_blank\">Viewer</a>"
      ],
      "text/plain": [
       "http://127.0.0.1:58455/v/9d16af3d4b925d1014b2a86b150b63f39a39731e/"
      ]
     },
     "execution_count": 1009,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viewer = neuroglancer.Viewer()\n",
    "with viewer.txn() as s:\n",
    "    s.layers['image'] = neuroglancer.ImageLayer(source='brainmaps://10393113184:ell:roi450um_xyz')\n",
    "    s.layers['segmentation'] = neuroglancer.SegmentationLayer(\n",
    "        source='brainmaps://10393113184:ell:roi450um_seg32fb16fb_220930', \n",
    "        selected_alpha=0.5\n",
    "    )\n",
    "\n",
    "# Initialize a neurites counter\n",
    "viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1036,
   "metadata": {},
   "outputs": [],
   "source": [
    "element = list(dup_dict.keys())[5]\n",
    "segs = [int(v) for v in dup_dict[element]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1037,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this element is 371524256\n",
      "Remaining neurites 3579\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define color mapping for the segment(s)\n",
    "color_value = \"#ef8200\"\n",
    "segment_color_mapping = {key: generate_random_hex_color() for key in segs}\n",
    "\n",
    "# Update the viewer\n",
    "with viewer.txn() as s:\n",
    "    # Assign segments using `set()`, since `.segments` is a Neuroglancer `VisibleSegments` object\n",
    "    s.layers['segmentation'].segments = set(segs)\n",
    "    s.layers['segmentation'].segment_colors = segment_color_mapping\n",
    "    s.layers['segmentation'].visible = True\n",
    "\n",
    "print(f'this element is {element}')\n",
    "print('Remaining neurites '+ str(len(neurites)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1053,
   "metadata": {},
   "outputs": [],
   "source": [
    "proofread_list_og = [item for dictionary in proofread_objects for sublist in dictionary.values() for item in sublist]\n",
    "proofread_list_kp = set([num for k,v in proofread_objects_dict.items() for num in v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1054,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 1054,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(proofread_list_og).difference(set(proofread_list_kp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1048,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dictionary of proofread objects from json list\n",
    "proofread_objects_dict = {k: v for l in proofread_objects for k, v in l.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1052,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each duplicate key, replace values with full set from combined dup_dict\n",
    "for k,v in dup_dict.items():\n",
    "    proofread_objects_dict[k]=v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 995,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 995,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set([k for l in proofread_objects for k, v in l.items()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 1 original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALIZE #\n",
    "\n",
    "filename_ref_list = \"efish_box1_segments_list.csv\"\n",
    "filename_proofread_list = 'box1_split_corrected.json'\n",
    "\n",
    "# Grab the reference list segments (all segments in the box >1000 voxels)\n",
    "with open(filename_ref_list, 'r') as csv_file:\n",
    "    reader = csv.reader(csv_file)\n",
    "    ref_list = [int(value) for row in reader for value in row]  # Flattened list of uint64 values\n",
    "\n",
    "# Grab the split-corrected segments so far\n",
    "if os.path.exists(filename_proofread_list):\n",
    "    with open(filename_proofread_list, 'r') as f:\n",
    "        proofread_objects = json.load(f)\n",
    "else:\n",
    "    with open(filename_proofread_list, 'w') as outfile:  \n",
    "        json.dump([], outfile)\n",
    "    proofread_objects = []\n",
    "\n",
    "# Extract proofread segment IDs (make it a flat list) and subtract it from the remaining list\n",
    "proofread_list = [item for dictionary in proofread_objects for sublist in dictionary.values() for item in sublist]\n",
    "remaining_list = [seg_id for seg_id in ref_list if seg_id not in proofread_list]\n",
    "neurites = remaining_list\n",
    "ntotal = len(neurites)\n",
    "print('Remaining neurites '+ str(len(neurites)))\n",
    "\n",
    "viewer = neuroglancer.Viewer()\n",
    "with viewer.txn() as s:\n",
    "    s.layers['image'] = neuroglancer.ImageLayer(source='brainmaps://10393113184:ell:roi450um_xyz')\n",
    "    s.layers['segmentation'] = neuroglancer.SegmentationLayer(\n",
    "        source='brainmaps://10393113184:ell:roi450um_seg32fb16fb_220930', \n",
    "        selected_alpha=0.5\n",
    "    )\n",
    "\n",
    "# Initialize a neurites counter\n",
    "viewer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP2 original "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 933,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining neurites 3579\n"
     ]
    }
   ],
   "source": [
    "##################################################################################################\n",
    "# SAVE\n",
    "# print the split-corrected segments\n",
    "segments = list((viewer.state.layers['segmentation'].segments))\n",
    "for i in range( len(segments)):\n",
    "    segments[i] = int(segments[i])\n",
    "       \n",
    "\n",
    "dict_seg = {str(segments[0]): segments}\n",
    "\n",
    "# save the selected segments\n",
    "proofread_objects.append(dict_seg)\n",
    "with open(filename_proofread_list, 'w') as outfile:  \n",
    "        json.dump(proofread_objects, outfile)\n",
    "\n",
    "################################################################################################\n",
    "# RESET \n",
    "# Grab the reference list segments (all segments in the box >1000 voxels)\n",
    "with open(filename_ref_list, 'r') as csv_file:\n",
    "    reader = csv.reader(csv_file)\n",
    "    ref_list = [int(value) for row in reader for value in row]  # Flattened list of uint64 values\n",
    "\n",
    "# Grab the split-corrected segments so far\n",
    "if os.path.exists(filename_proofread_list):\n",
    "    with open(filename_proofread_list, 'r') as f:\n",
    "        proofread_objects = json.load(f)\n",
    "else:\n",
    "    with open(filename_proofread_list, 'w') as outfile:  \n",
    "        json.dump([], outfile)\n",
    "    proofread_objects = []\n",
    "\n",
    "# Extract proofread segment IDs (make it a flat list) and subtract it from the remaining list\n",
    "proofread_list = [item for dictionary in proofread_objects for sublist in dictionary.values() for item in sublist]\n",
    "remaining_list = [seg_id for seg_id in ref_list if seg_id not in proofread_list]\n",
    "neurites = remaining_list\n",
    "ntotal = len(neurites)\n",
    "print('Remaining neurites '+ str(len(neurites)))\n",
    "\n",
    "# Remove all segments from the \"segmentation\" layer\n",
    "with viewer.txn() as txn:\n",
    "    layer = txn.layers['segmentation']\n",
    "    layer.segments = set()  # Assign an empty set to clear the segments\n",
    "\n",
    "with viewer.txn(overwrite=True) as s:\n",
    "    s.layers['Base Segment Merger'] = neuroglancer.AnnotationLayer()\n",
    "    s.layers['Base Segment Merger'].filterBySegmentation = [\"segments\"]\n",
    "    s.layers['Base Segment Merger'].linkedSegmentationLayer = {\"segments\": 'segmentation'}\n",
    "    s.layers['Base Segment Merger'].annotationColor = '#ffa500'\n",
    "    s.layers['Base Segment Merger'].tool = \"annotatePoint\"\n",
    "    \n",
    "\n",
    "point_type = 'ends'\n",
    "with viewer.txn(overwrite=True) as s:\n",
    "    s.layers[point_type] = neuroglancer.AnnotationLayer()\n",
    "    s.layers[point_type].annotationColor = '#ffff00'\n",
    "    s.layers[point_type].tool = \"annotatePoint\"\n",
    "    s.layers[point_type].tab = 'Annotations'\n",
    "\n",
    "# RUN STEP1 again\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# other\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "vx_sizes = [16,16,30]\n",
    "t = 'Base Segment Merger'\n",
    "this_type_points = []\n",
    "for x in viewer.state.layers[t].annotations:\n",
    "    if x.segments == None:\n",
    "        c = [int(y) for y in x.point]\n",
    "        print(f'Error, no segment for point {c}, for point layer {t}, correct and re-save')\n",
    "    \n",
    "    else:\n",
    "        co_ords = [float(x) for x in list(x.point)]\n",
    "        co_ords_and_id = ([co_ords[x]*vx_sizes[x] for x in range(3)])\n",
    "        co_ords_and_id.append(str(x.segments[0][0]))\n",
    "        \n",
    "        this_type_points.append((co_ords_and_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(this_type_points,columns = ['x','y','z','segID'])\n",
    "df.to_csv('Base_Segment_Merger.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Base_Segment_Merger.csv')\n",
    "base_seg_merge_points = df[['x','y','z','segID']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pos,point in enumerate(base_seg_merge_points):\n",
    "    point_array = np.asarray([int(point[x]/vx_sizes[x]) for x in range(3)])\n",
    "    pa = neuroglancer.PointAnnotation(id=f'bm_{pos}', point = point_array, segments=[[point[3]]])\n",
    "    s.layers['Base Segment Merger'].annotations.append(pa)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filename_proofread_list, 'r') as f:\n",
    "    proofread_objects = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dict = {}\n",
    "for i in proofread_objects:\n",
    "    for k,v in i.items():\n",
    "        split_dict[k]=v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "proofread_glia_segs_0 = set(split_dict['286796800'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "proofread_glia_segs_1 = set(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments = list(proofread_glia_segs_1.union(proofread_glia_segs_0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
